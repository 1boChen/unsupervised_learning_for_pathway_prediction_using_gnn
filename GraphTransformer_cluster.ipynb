{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4816\n",
      "Number of edges: 101782\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "def build_multidigraph_from_csv(csv_file):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            # Add nodes (if not already added)\n",
    "            G.add_node(row['starter_ID'], name=row['starter_ID'])\n",
    "            G.add_node(row['receiver_ID'], name=row['receiver_ID'])\n",
    "\n",
    "            # Add directed edges with additional attributes\n",
    "            # Each edge is unique and can represent a different type of interaction\n",
    "            G.add_edge(\n",
    "                row['starter_ID'], \n",
    "                row['receiver_ID'], \n",
    "                interaction_type=row['subtype_name'],\n",
    "                relation_type=row['relation_type'],\n",
    "                pathway_sources=row['pathway_source'],\n",
    "                credibility=row['credibility']\n",
    "            )\n",
    "\n",
    "    return G\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'relations_train_final.csv'  # Update this to the path of your relations_train.csv file\n",
    "\n",
    "# Build the multidigraph\n",
    "MDG = build_multidigraph_from_csv(csv_file_path)\n",
    "\n",
    "# Print basic information about the multidigraph\n",
    "print(f\"Number of nodes: {MDG.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {MDG.number_of_edges()}\")\n",
    "\n",
    "# The multidigraph `MDG` is now constructed with nodes and multiple types of directed edges from the relations_train.csv\n",
    "# You can now use `MDG` for further analysis or as input to clustering algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_cluster_assignments(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    gene_to_cluster = {row['Gene']: row['Cluster'] for _, row in df.iterrows()}\n",
    "    return gene_to_cluster\n",
    "\n",
    "# Use this function to read your CSV file\n",
    "gene_to_cluster = read_cluster_assignments('vgae_spectral_gene_cluster_assignments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "\n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, num_classes, cluster_feature_dim = 8, num_hidden_units=32, num_heads=5):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.node_emb = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "        self.cluster_emb = torch.nn.Embedding(num_clusters, cluster_feature_dim)  # Add cluster embeddings\n",
    "\n",
    "        # First Graph Transformer layer\n",
    "        self.conv1 = TransformerConv(embedding_dim + cluster_feature_dim, num_hidden_units, heads=num_heads, dropout=0.6, edge_dim=None)\n",
    "        \n",
    "        # Output layer\n",
    "        self.conv2 = TransformerConv(num_hidden_units * num_heads, num_classes, heads=1, concat=True, dropout=0.6, edge_dim=None)\n",
    "\n",
    "    def forward(self, data, cluster_indices):\n",
    "        x = self.node_emb(data.node_index)\n",
    "        cluster_x = self.cluster_emb(cluster_indices)  # Embedding for cluster\n",
    "        x = torch.cat([x, cluster_x], dim=1)  # Concatenate node and cluster embeddings\n",
    "\n",
    "        edge_index = data.edge_index\n",
    "        edge_weight = None  # Update if you have edge weights\n",
    "\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Assuming MDG is your MultiDiGraph\n",
    "train_data = from_networkx(MDG)\n",
    "\n",
    "num_nodes = train_data.num_nodes\n",
    "embedding_dim = 32  # Choose an appropriate embedding size\n",
    "num_classes = 7   # Set the number of classes based on your edge types\n",
    "num_clusters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all unique interaction types from MDG\n",
    "interaction_types = set()\n",
    "for _, _, edge_data in MDG.edges(data=True):\n",
    "    interaction_types.add(edge_data['interaction_type'])\n",
    "\n",
    "# Update the mapping to include 'no interaction' class\n",
    "interaction_type_to_label = {inter_type: i for i, inter_type in enumerate(interaction_types)}\n",
    "num_classes = len(interaction_type_to_label)\n",
    "\n",
    "def setup_edge_labels_with_no_interaction(MDG, interaction_type_to_label, data):\n",
    "    num_nodes = len(MDG.nodes())\n",
    "    edge_labels = torch.zeros((num_nodes, num_nodes), dtype=torch.long)\n",
    "\n",
    "    for u, v, edge_data in MDG.edges(data=True):\n",
    "        u_index = list(MDG.nodes()).index(u)\n",
    "        v_index = list(MDG.nodes()).index(v)\n",
    "        label = interaction_type_to_label[edge_data['interaction_type']]\n",
    "        edge_labels[u_index, v_index] = label\n",
    "\n",
    "    data.edge_label = edge_labels  # Directly modify the data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nodes_to_clusters(G, gene_to_cluster):\n",
    "    # Assuming gene_to_cluster is the dictionary from gene to cluster ID\n",
    "    node_to_cluster = {}\n",
    "    for node in G.nodes():\n",
    "        cluster_id = gene_to_cluster.get(node, 99)  # default_cluster_id can be set to a specific value\n",
    "        node_to_cluster[node] = cluster_id\n",
    "\n",
    "    return node_to_cluster\n",
    "\n",
    "node_to_cluster = map_nodes_to_clusters(MDG, gene_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Process training data\n",
    "val_MDG = build_multidigraph_from_csv('cleaned_relations_val_final.csv')\n",
    "\n",
    "# Create a global node to index mapping\n",
    "all_nodes = set(MDG.nodes()).union(set(val_MDG.nodes()))\n",
    "global_node_to_index = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "\n",
    "# Function to map nodes in a graph to global indices\n",
    "def map_nodes_to_global_indices(G, global_node_to_index):\n",
    "    return [global_node_to_index[node] for node in G.nodes()]\n",
    "\n",
    "val_data = from_networkx(val_MDG)\n",
    "\n",
    "train_data.node_index = torch.tensor(map_nodes_to_global_indices(MDG, global_node_to_index), dtype=torch.long)\n",
    "\n",
    "cluster_indices = [node_to_cluster[node] for node in MDG.nodes()]\n",
    "cluster_indices_tensor = torch.tensor(cluster_indices, dtype=torch.long)\n",
    "\n",
    "val_data.node_index = torch.tensor(map_nodes_to_global_indices(val_MDG, global_node_to_index), dtype=torch.long)\n",
    "setup_edge_labels_with_no_interaction(val_MDG, interaction_type_to_label, val_data)\n",
    "setup_edge_labels_with_no_interaction(MDG, interaction_type_to_label, train_data)\n",
    "val_node_to_cluster = map_nodes_to_clusters(val_MDG, gene_to_cluster)\n",
    "\n",
    "# Convert the node-cluster mapping to a tensor for the validation set\n",
    "val_cluster_indices = [val_node_to_cluster[node] for node in val_MDG.nodes()]\n",
    "val_cluster_indices_tensor = torch.tensor(val_cluster_indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "model = GraphTransformer(num_nodes, embedding_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dataset of edges and their labels\n",
    "edges = train_data.edge_index.t().tolist()  # List of [node_u, node_v]\n",
    "labels = [train_data.edge_label[edge[0], edge[1]].item() for edge in edges]\n",
    "\n",
    "edge_dataset = list(zip(edges, labels))\n",
    "edge_loader = DataLoader(edge_dataset, batch_size=1024, shuffle=True)  # Adjust batch_size as needed\n",
    "\n",
    "# Assuming val_data is structured similarly to train_data\n",
    "val_edges = val_data.edge_index.t().tolist()\n",
    "val_labels = [val_data.edge_label[edge[0], edge[1]].item() for edge in val_edges]\n",
    "\n",
    "val_edge_dataset = list(zip(val_edges, val_labels))\n",
    "val_edge_loader = DataLoader(val_edge_dataset, batch_size=1024, shuffle=False)  # You can adjust the batch size\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "inverse_interaction_type_to_label = {v: k for k, v in interaction_type_to_label.items()}\n",
    "\n",
    "def validate(val_data, val_cluster_indices_tensor, device, model, criterion):\n",
    "    model.eval()\n",
    "    total_cross_entropy_loss = 0  # Initialize cross-entropy loss\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_edge_loader, desc='Validating'):\n",
    "            edge_tensors, label_batch = batch\n",
    "            node_u_list, node_v_list = edge_tensors[0].to(device), edge_tensors[1].to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            # Pass both val_data and val_cluster_indices_tensor to the model\n",
    "            output = model(val_data.to(device), val_cluster_indices_tensor.to(device))\n",
    "            edge_predictions = output[node_u_list].argmax(dim=1)  # Predicted classes\n",
    "\n",
    "            # Calculate and accumulate cross-entropy loss\n",
    "            cross_entropy_loss = criterion(output[node_u_list], label_batch)\n",
    "            total_cross_entropy_loss += cross_entropy_loss.item()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            preds = edge_predictions.cpu().numpy()\n",
    "            true_labels = label_batch.cpu().numpy()\n",
    "            all_predictions.extend(preds)\n",
    "            all_true_labels.extend(true_labels)\n",
    "\n",
    "    # Calculate average cross-entropy loss over all batches\n",
    "    avg_cross_entropy_loss = total_cross_entropy_loss / len(val_edge_loader)\n",
    "\n",
    "    # Calculate F1 Score and classification report\n",
    "    weighted_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "    class_report = classification_report(all_true_labels, all_predictions, target_names=[inverse_interaction_type_to_label[i] for i in range(num_classes)], zero_division=0)\n",
    "\n",
    "    print(f\"Weighted F1 Score: {weighted_f1}\\nClassification Report:\\n{class_report}\")\n",
    "    print(f\"Validation Cross-Entropy Loss: {avg_cross_entropy_loss}\")\n",
    "\n",
    "    return weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = GraphTransformer(num_nodes, embedding_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Learning rate\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Loss function\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move your model to the chosen device\n",
    "model = model.to(device)\n",
    "\n",
    "def train(train_data, cluster_indices_tensor, model, optimizer, criterion, edge_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(edge_loader, desc='Training'):\n",
    "        edge_tensors, label_batch = batch\n",
    "        \n",
    "        node_u_list, node_v_list = edge_tensors[0].to(device), edge_tensors[1].to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass both train_data and cluster_indices_tensor to the model\n",
    "        output = model(train_data.to(device), cluster_indices_tensor.to(device))\n",
    "\n",
    "        edge_predictions = output[node_u_list]  # Corrected indexing\n",
    "        \n",
    "        loss = criterion(edge_predictions, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(edge_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:47<00:00,  1.07s/it]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.301333448509136\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.07      0.23      0.11       694\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.17      0.09      0.12      1745\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         expression       0.01      0.01      0.01       722\n",
      "        no_relation       0.33      0.36      0.34      6143\n",
      "         activation       0.45      0.39      0.42      7342\n",
      "\n",
      "           accuracy                           0.30     18128\n",
      "          macro avg       0.15      0.15      0.14     18128\n",
      "       weighted avg       0.31      0.30      0.30     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.8203808400366042\n",
      "Epoch 1, Train Loss: 2.0640, Val F1 Score: 0.3013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [02:13<00:00,  1.34s/it]\n",
      "Validating: 100%|██████████| 18/18 [00:04<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3131913906461618\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.10      0.27      0.15       694\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.15      0.04      0.07      1745\n",
      "binding/association       0.01      0.01      0.01       458\n",
      "         expression       0.00      0.00      0.00       722\n",
      "        no_relation       0.33      0.51      0.40      6143\n",
      "         activation       0.47      0.36      0.40      7342\n",
      "\n",
      "           accuracy                           0.33     18128\n",
      "          macro avg       0.15      0.17      0.15     18128\n",
      "       weighted avg       0.32      0.33      0.31     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.753282454278734\n",
      "Epoch 2, Train Loss: 1.8729, Val F1 Score: 0.3132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [02:29<00:00,  1.49s/it]\n",
      "Validating: 100%|██████████| 18/18 [00:04<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3247708865230815\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.10      0.21      0.13       694\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.15      0.04      0.07      1745\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         expression       0.00      0.00      0.00       722\n",
      "        no_relation       0.34      0.54      0.41      6143\n",
      "         activation       0.49      0.38      0.43      7342\n",
      "\n",
      "           accuracy                           0.35     18128\n",
      "          macro avg       0.15      0.17      0.15     18128\n",
      "       weighted avg       0.33      0.35      0.32     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7242636879285176\n",
      "Epoch 3, Train Loss: 1.8035, Val F1 Score: 0.3248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [02:28<00:00,  1.48s/it]\n",
      "Validating: 100%|██████████| 18/18 [00:04<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3268992942708247\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.10      0.14      0.12       694\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.02      0.00      0.01      1745\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         expression       0.00      0.00      0.00       722\n",
      "        no_relation       0.34      0.60      0.43      6143\n",
      "         activation       0.49      0.38      0.43      7342\n",
      "\n",
      "           accuracy                           0.36     18128\n",
      "          macro avg       0.14      0.16      0.14     18128\n",
      "       weighted avg       0.32      0.36      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7072468201319377\n",
      "Epoch 4, Train Loss: 1.7617, Val F1 Score: 0.3269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:37<00:00,  1.02it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3361291473667383\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.11      0.12      0.12       694\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.05      0.01      0.02      1745\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         expression       0.00      0.00      0.00       722\n",
      "        no_relation       0.34      0.61      0.44      6143\n",
      "         activation       0.50      0.41      0.45      7342\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.14      0.16      0.15     18128\n",
      "       weighted avg       0.33      0.38      0.34     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6949044797155592\n",
      "Epoch 5, Train Loss: 1.7317, Val F1 Score: 0.3361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3335472102394604\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.09      0.07      0.08       694\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.06      0.01      0.01      1745\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         expression       0.00      0.00      0.00       722\n",
      "        no_relation       0.34      0.61      0.44      6143\n",
      "         activation       0.49      0.41      0.45      7342\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.14      0.16      0.14     18128\n",
      "       weighted avg       0.32      0.38      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6830394665400188\n",
      "Epoch 6, Train Loss: 1.7117, Val F1 Score: 0.3335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3289178280006899\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.08      0.05      0.06       694\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.05      0.00      0.01      1745\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         expression       0.00      0.00      0.00       722\n",
      "        no_relation       0.34      0.62      0.44      6143\n",
      "         activation       0.47      0.41      0.44      7342\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.13      0.15      0.14     18128\n",
      "       weighted avg       0.31      0.38      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6744003958172269\n",
      "Epoch 7, Train Loss: 1.6917, Val F1 Score: 0.3289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.32448546234244224\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.02      0.01      0.01       694\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.05      0.00      0.01      1745\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         expression       0.00      0.00      0.00       722\n",
      "        no_relation       0.34      0.64      0.44      6143\n",
      "         activation       0.48      0.39      0.43      7342\n",
      "\n",
      "           accuracy                           0.37     18128\n",
      "          macro avg       0.13      0.15      0.13     18128\n",
      "       weighted avg       0.31      0.37      0.32     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6649342974026997\n",
      "Epoch 8, Train Loss: 1.6734, Val F1 Score: 0.3245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.32624435394449297\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.05      0.02      0.03       694\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         expression       0.00      0.00      0.00       722\n",
      "        no_relation       0.34      0.65      0.45      6143\n",
      "         activation       0.48      0.39      0.43      7342\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.12      0.15      0.13     18128\n",
      "       weighted avg       0.31      0.38      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6566992402076721\n",
      "Epoch 9, Train Loss: 1.6551, Val F1 Score: 0.3262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3310854010385152\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.06      0.02      0.03       694\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         expression       0.00      0.00      0.00       722\n",
      "        no_relation       0.34      0.66      0.45      6143\n",
      "         activation       0.49      0.40      0.44      7342\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.13      0.15      0.13     18128\n",
      "       weighted avg       0.32      0.38      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6487548285060458\n",
      "Epoch 10, Train Loss: 1.6461, Val F1 Score: 0.3311\n",
      "Early stopping triggered\n",
      "Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Early stopping and scheduler parameters\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "best_f1_score = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Call train function with necessary parameters\n",
    "    train_loss = train(train_data, cluster_indices_tensor, model, optimizer, criterion, edge_loader, device)\n",
    "\n",
    "    # Call validate function with necessary parameters\n",
    "    val_f1_score = validate(val_data, val_cluster_indices_tensor, device, model, criterion)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val F1 Score: {val_f1_score:.4f}')\n",
    "\n",
    "    # Step the scheduler based on validation F1 score\n",
    "    scheduler.step(val_f1_score)\n",
    "\n",
    "    # Check for improvement in validation F1 score\n",
    "    if val_f1_score > best_f1_score:\n",
    "        best_f1_score = val_f1_score\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()  # Save the best model state\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Save the best model state\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'new_GTC_unfreeze_32_32_5_best.pth')\n",
    "    print(\"Best model saved.\")\n",
    "else:\n",
    "    print(\"No model improvement was observed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_siamese_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
