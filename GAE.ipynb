{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '6'  # Adjust the number of threads as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def build_multidigraph_from_csv(csv_file):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            # Exclude 'no_relation' edges\n",
    "            if row['relation_type'] != 'no_relation':\n",
    "                # Add nodes with the 'name' attribute\n",
    "                G.add_node(row['starter_ID'], name=row['starter_ID'])\n",
    "                G.add_node(row['receiver_ID'], name=row['receiver_ID'])\n",
    "\n",
    "                # Add directed edges with additional attributes\n",
    "                weight = float(row['weight'])\n",
    "                G.add_edge(\n",
    "                    row['starter_ID'], \n",
    "                    row['receiver_ID'], \n",
    "                    weight=weight,\n",
    "                    interaction_type=row['subtype_name'],\n",
    "                    relation_type=row['relation_type'],\n",
    "                    pathway_sources=row['pathway_source'],\n",
    "                    credibility=row['credibility']\n",
    "                )\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Paths to the CSV files\n",
    "train_csv_path = 'relations_train_final.csv'\n",
    "val_csv_path = 'cleaned_relations_val_final.csv'\n",
    "\n",
    "# Build the MultiDiGraphs\n",
    "train_MDG = build_multidigraph_from_csv(train_csv_path)\n",
    "val_MDG = build_multidigraph_from_csv(val_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx\n",
    "import torch\n",
    "\n",
    "# Create a node to index mapping\n",
    "node_to_index = {node: i for i, node in enumerate(train_MDG.nodes())}\n",
    "\n",
    "# Initialize node features with identity matrix\n",
    "num_nodes = len(train_MDG.nodes())\n",
    "node_features = torch.eye(num_nodes)\n",
    "\n",
    "# Convert to PyTorch Geometric Data\n",
    "train_data = from_networkx(train_MDG)\n",
    "train_data.x = node_features\n",
    "\n",
    "# Convert the validation graph to PyTorch Geometric data\n",
    "val_data = from_networkx(val_MDG)\n",
    "val_data.x = torch.eye(len(val_MDG.nodes()))  # Use identity matrix as features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "from torch_geometric.nn.models import GAE\n",
    "\n",
    "class GATGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GATGCNEncoder, self).__init__()\n",
    "        # First layer is GCN\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        # Second layer is GAT\n",
    "        self.conv2 = GATConv(2 * out_channels, out_channels, heads=1, dropout=0.2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "# Example usage\n",
    "in_channels = num_nodes  # Adjust as per your data\n",
    "out_channels = 16  # Embedding size\n",
    "\n",
    "encoder = GATGCNEncoder(in_channels, out_channels)\n",
    "model = GAE(encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "\n",
    "def validate(model, node_to_index, val_nodes, initial_step=10, detailed_step=1, max_clusters=1000):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(train_data.x, train_data.edge_index)\n",
    "        z_np = z.cpu().numpy()\n",
    "\n",
    "    val_embeddings = np.array([z_np[node_to_index[node]] for node in val_nodes])\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    umap_embeddings = reducer.fit_transform(val_embeddings)\n",
    "\n",
    "    # Initial broad search\n",
    "    broad_best_score, broad_best_n_clusters = broad_search(umap_embeddings, initial_step, max_clusters)\n",
    "\n",
    "    # Detailed search within promising range\n",
    "    start = max(2, broad_best_n_clusters - initial_step)\n",
    "    end = min(broad_best_n_clusters + initial_step, max_clusters)\n",
    "    best_score, best_n_clusters = detailed_search(umap_embeddings, start, end, detailed_step)\n",
    "\n",
    "    print(f\"Best silhouette score: {best_score} for {best_n_clusters} clusters\")\n",
    "    return best_score\n",
    "\n",
    "def broad_search(embeddings, step, max_clusters):\n",
    "    best_score = -1\n",
    "    best_n_clusters = 0\n",
    "    for n_clusters in range(2, max_clusters + 1, step):\n",
    "        score = calculate_silhouette_score(embeddings, n_clusters)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n_clusters = n_clusters\n",
    "    return best_score, best_n_clusters\n",
    "\n",
    "def detailed_search(embeddings, start, end, step):\n",
    "    best_score = -1\n",
    "    best_n_clusters = 0\n",
    "    for n_clusters in range(start, end + 1, step):\n",
    "        score = calculate_silhouette_score(embeddings, n_clusters)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n_clusters = n_clusters\n",
    "    return best_score, best_n_clusters\n",
    "\n",
    "def calculate_silhouette_score(embeddings, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    return silhouette_score(embeddings, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best silhouette score: 0.6488627195358276 for 75 clusters\n",
      "Epoch: 1, Loss: 1.3863, Val Score: 0.6489\n",
      "Best silhouette score: 0.6517704129219055 for 72 clusters\n",
      "Epoch: 2, Loss: 1.3846, Val Score: 0.6518\n",
      "Best silhouette score: 0.6490039825439453 for 89 clusters\n",
      "Epoch: 3, Loss: 1.3732, Val Score: 0.6490\n",
      "Best silhouette score: 0.6540904641151428 for 102 clusters\n",
      "Epoch: 4, Loss: 1.3451, Val Score: 0.6541\n",
      "Best silhouette score: 0.662098228931427 for 172 clusters\n",
      "Epoch: 5, Loss: 1.3003, Val Score: 0.6621\n",
      "Best silhouette score: 0.6646788120269775 for 86 clusters\n",
      "Epoch: 6, Loss: 1.2368, Val Score: 0.6647\n",
      "Best silhouette score: 0.6597326993942261 for 118 clusters\n",
      "Epoch: 7, Loss: 1.1993, Val Score: 0.6597\n",
      "Best silhouette score: 0.6748242378234863 for 162 clusters\n",
      "Epoch: 8, Loss: 1.1869, Val Score: 0.6748\n",
      "Best silhouette score: 0.6747502088546753 for 60 clusters\n",
      "Epoch: 9, Loss: 1.1686, Val Score: 0.6748\n",
      "Best silhouette score: 0.6844045519828796 for 86 clusters\n",
      "Epoch: 10, Loss: 1.1199, Val Score: 0.6844\n",
      "Best silhouette score: 0.6878721714019775 for 93 clusters\n",
      "Epoch: 11, Loss: 1.0924, Val Score: 0.6879\n",
      "Best silhouette score: 0.6798058748245239 for 104 clusters\n",
      "Epoch: 12, Loss: 1.0772, Val Score: 0.6798\n",
      "Best silhouette score: 0.6934163570404053 for 86 clusters\n",
      "Epoch: 13, Loss: 1.0705, Val Score: 0.6934\n",
      "Best silhouette score: 0.6953031420707703 for 82 clusters\n",
      "Epoch: 14, Loss: 1.0527, Val Score: 0.6953\n",
      "Best silhouette score: 0.6994837522506714 for 99 clusters\n",
      "Epoch: 15, Loss: 1.0428, Val Score: 0.6995\n",
      "Best silhouette score: 0.6898189187049866 for 92 clusters\n",
      "Epoch: 16, Loss: 1.0271, Val Score: 0.6898\n",
      "Best silhouette score: 0.6799158453941345 for 68 clusters\n",
      "Epoch: 17, Loss: 1.0307, Val Score: 0.6799\n",
      "Best silhouette score: 0.6869407892227173 for 86 clusters\n",
      "Epoch: 18, Loss: 1.0133, Val Score: 0.6869\n",
      "Best silhouette score: 0.7024964094161987 for 79 clusters\n",
      "Epoch: 19, Loss: 1.0003, Val Score: 0.7025\n",
      "Best silhouette score: 0.6851049661636353 for 77 clusters\n",
      "Epoch: 20, Loss: 0.9874, Val Score: 0.6851\n",
      "Best silhouette score: 0.6840468645095825 for 78 clusters\n",
      "Epoch: 21, Loss: 0.9816, Val Score: 0.6840\n",
      "Best silhouette score: 0.6774433255195618 for 91 clusters\n",
      "Epoch: 22, Loss: 0.9649, Val Score: 0.6774\n",
      "Best silhouette score: 0.6744880676269531 for 68 clusters\n",
      "Epoch: 23, Loss: 0.9615, Val Score: 0.6745\n",
      "Best silhouette score: 0.6732941269874573 for 70 clusters\n",
      "Epoch: 24, Loss: 0.9655, Val Score: 0.6733\n",
      "Best silhouette score: 0.6849380135536194 for 107 clusters\n",
      "Epoch: 25, Loss: 0.9665, Val Score: 0.6849\n",
      "Best silhouette score: 0.6810188293457031 for 82 clusters\n",
      "Epoch: 26, Loss: 0.9626, Val Score: 0.6810\n",
      "Best silhouette score: 0.678229033946991 for 102 clusters\n",
      "Epoch: 27, Loss: 0.9685, Val Score: 0.6782\n",
      "Best silhouette score: 0.6836415529251099 for 59 clusters\n",
      "Epoch: 28, Loss: 0.9605, Val Score: 0.6836\n",
      "Best silhouette score: 0.682996392250061 for 74 clusters\n",
      "Epoch: 29, Loss: 0.9587, Val Score: 0.6830\n",
      "Early stopping triggered\n",
      "Stopped early due to no improvement\n",
      "Best model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Early stopping and model saving parameters\n",
    "patience = 10\n",
    "best_val_score = -1\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "best_model_state = None  # To store the best model state\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=5, factor=0.5)\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    val_nodes = list(val_MDG.nodes())\n",
    "    val_score = validate(model, node_to_index, val_nodes)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Val Score: {val_score:.4f}')\n",
    "\n",
    "    scheduler.step(val_score)\n",
    "\n",
    "    # Save model if validation score improved\n",
    "    if val_score > best_val_score:\n",
    "        best_val_score = val_score\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()  # Save the best model state\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        early_stop = True\n",
    "        break\n",
    "\n",
    "if early_stop:\n",
    "    print(\"Stopped early due to no improvement\")\n",
    "\n",
    "# Save the best model state to a file after the training loop\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'best_gae_model.pth')\n",
    "    print(\"Best model saved.\")\n",
    "else:\n",
    "    print(\"No model improvement was observed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.6981, Val Loss: 1.6455\n",
      "Epoch: 2, Loss: 1.4270, Val Loss: 1.5079\n",
      "Epoch: 3, Loss: 1.3376, Val Loss: 1.4548\n",
      "Epoch: 4, Loss: 1.3223, Val Loss: 1.4393\n",
      "Epoch: 5, Loss: 1.3106, Val Loss: 1.4387\n",
      "Epoch: 6, Loss: 1.3036, Val Loss: 1.4335\n",
      "Epoch: 7, Loss: 1.2900, Val Loss: 1.4346\n",
      "Epoch: 8, Loss: 1.2818, Val Loss: 1.4299\n",
      "Epoch: 9, Loss: 1.2578, Val Loss: 1.4169\n",
      "Epoch: 10, Loss: 1.2504, Val Loss: 1.3958\n",
      "Epoch: 11, Loss: 1.2331, Val Loss: 1.3833\n",
      "Epoch: 12, Loss: 1.2171, Val Loss: 1.3745\n",
      "Epoch: 13, Loss: 1.1938, Val Loss: 1.3747\n",
      "Epoch: 14, Loss: 1.1852, Val Loss: 1.3738\n",
      "Epoch: 15, Loss: 1.1765, Val Loss: 1.3730\n",
      "Epoch: 16, Loss: 1.1562, Val Loss: 1.3818\n",
      "Epoch: 17, Loss: 1.1449, Val Loss: 1.3811\n",
      "Epoch: 18, Loss: 1.1403, Val Loss: 1.3890\n",
      "Epoch: 19, Loss: 1.1235, Val Loss: 1.3969\n",
      "Epoch: 20, Loss: 1.1225, Val Loss: 1.4065\n",
      "Epoch: 21, Loss: 1.1155, Val Loss: 1.4069\n",
      "Epoch: 22, Loss: 1.1004, Val Loss: 1.4180\n",
      "Epoch: 23, Loss: 1.1046, Val Loss: 1.4144\n",
      "Epoch: 24, Loss: 1.0984, Val Loss: 1.4170\n",
      "Epoch: 25, Loss: 1.0936, Val Loss: 1.4202\n",
      "Early stopping triggered\n",
      "Best model saved.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('gae_sihoulette_score.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_embeddings = model.encode(train_data.x, train_data.edge_index).cpu().numpy()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 92  # Set the number of clusters, or determine it based on your criteria\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "train_cluster_labels = kmeans.fit_predict(train_embeddings)\n",
    "\n",
    "updated_data = []\n",
    "# Read the existing CSV file and add the new cluster assignments\n",
    "existing_data = {}\n",
    "with open('gene_cluster_assignments.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)  # Skip the header\n",
    "\n",
    "    for row in reader:\n",
    "        gene = row[0]\n",
    "        existing_data[gene] = row\n",
    "\n",
    "# Add new cluster assignments based on gene names\n",
    "for gene, index in node_to_index.items():\n",
    "    if gene in existing_data:\n",
    "        new_cluster = train_cluster_labels[index]\n",
    "        existing_data[gene].append(new_cluster)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "with open('updated_gene_cluster_assignments.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header + ['GAE_Silhouette_Cluster'])  # New header with 'GAE_Silhouette_Cluster'\n",
    "    for gene, row in existing_data.items():\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_siamese_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
