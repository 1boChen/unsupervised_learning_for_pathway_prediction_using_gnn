{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4816\n",
      "Number of edges: 101782\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "def build_multidigraph_from_csv(csv_file):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            # Add nodes (if not already added)\n",
    "            G.add_node(row['starter_ID'], name=row['starter_ID'])\n",
    "            G.add_node(row['receiver_ID'], name=row['receiver_ID'])\n",
    "\n",
    "            # Add directed edges with additional attributes\n",
    "            # Each edge is unique and can represent a different type of interaction\n",
    "            G.add_edge(\n",
    "                row['starter_ID'], \n",
    "                row['receiver_ID'], \n",
    "                interaction_type=row['subtype_name'],\n",
    "                relation_type=row['relation_type'],\n",
    "                pathway_sources=row['pathway_source'],\n",
    "                credibility=row['credibility']\n",
    "            )\n",
    "\n",
    "    return G\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'relations_train_final.csv'  # Update this to the path of your relations_train.csv file\n",
    "\n",
    "# Build the multidigraph\n",
    "MDG = build_multidigraph_from_csv(csv_file_path)\n",
    "\n",
    "# Print basic information about the multidigraph\n",
    "print(f\"Number of nodes: {MDG.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {MDG.number_of_edges()}\")\n",
    "\n",
    "# The multidigraph `MDG` is now constructed with nodes and multiple types of directed edges from the relations_train.csv\n",
    "# You can now use `MDG` for further analysis or as input to clustering algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_cluster_assignments(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    gene_to_cluster = {row['Gene']: row['Cluster'] for _, row in df.iterrows()}\n",
    "    return gene_to_cluster\n",
    "\n",
    "# Use this function to read your CSV file\n",
    "gene_to_cluster = read_cluster_assignments('vgae_spectral_gene_cluster_assignments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "\n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, num_classes, cluster_feature_dim = 8, num_hidden_units=32, num_heads=5):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.node_emb = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "        self.cluster_emb = torch.nn.Embedding(num_clusters, cluster_feature_dim)  # Add cluster embeddings\n",
    "\n",
    "        # First Graph Transformer layer\n",
    "        self.conv1 = TransformerConv(embedding_dim + cluster_feature_dim, num_hidden_units, heads=num_heads, dropout=0.6, edge_dim=None)\n",
    "        \n",
    "        # Output layer\n",
    "        self.conv2 = TransformerConv(num_hidden_units * num_heads, num_classes, heads=1, concat=True, dropout=0.6, edge_dim=None)\n",
    "\n",
    "    def forward(self, data, cluster_indices):\n",
    "        x = self.node_emb(data.node_index)\n",
    "        cluster_x = self.cluster_emb(cluster_indices)  # Embedding for cluster\n",
    "        x = torch.cat([x, cluster_x], dim=1)  # Concatenate node and cluster embeddings\n",
    "\n",
    "        edge_index = data.edge_index\n",
    "        edge_weight = None  # Update if you have edge weights\n",
    "\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Assuming MDG is your MultiDiGraph\n",
    "train_data = from_networkx(MDG)\n",
    "train_data.node_index = torch.arange(train_data.num_nodes)  # Node indexing\n",
    "\n",
    "num_nodes = train_data.num_nodes\n",
    "embedding_dim = 24  # Choose an appropriate embedding size\n",
    "num_classes = 7   # Set the number of classes based on your edge types\n",
    "num_clusters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all unique interaction types from MDG\n",
    "interaction_types = set()\n",
    "for _, _, edge_data in MDG.edges(data=True):\n",
    "    interaction_types.add(edge_data['interaction_type'])\n",
    "\n",
    "# Update the mapping to include 'no interaction' class\n",
    "interaction_type_to_label = {inter_type: i for i, inter_type in enumerate(interaction_types)}\n",
    "num_classes = len(interaction_type_to_label)\n",
    "\n",
    "def setup_edge_labels_with_no_interaction(MDG, interaction_type_to_label, data):\n",
    "    num_nodes = len(MDG.nodes())\n",
    "    edge_labels = torch.zeros((num_nodes, num_nodes), dtype=torch.long)\n",
    "\n",
    "    for u, v, edge_data in MDG.edges(data=True):\n",
    "        u_index = list(MDG.nodes()).index(u)\n",
    "        v_index = list(MDG.nodes()).index(v)\n",
    "        label = interaction_type_to_label[edge_data['interaction_type']]\n",
    "        edge_labels[u_index, v_index] = label\n",
    "\n",
    "    data.edge_label = edge_labels  # Directly modify the data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nodes_to_clusters(G, gene_to_cluster):\n",
    "    # Assuming gene_to_cluster is the dictionary from gene to cluster ID\n",
    "    node_to_cluster = {}\n",
    "    for node in G.nodes():\n",
    "        cluster_id = gene_to_cluster.get(node, 99)  # default_cluster_id can be set to a specific value\n",
    "        node_to_cluster[node] = cluster_id\n",
    "\n",
    "    return node_to_cluster\n",
    "\n",
    "node_to_cluster = map_nodes_to_clusters(MDG, gene_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Process training data\n",
    "train_data.node_index = torch.arange(train_data.num_nodes)\n",
    "setup_edge_labels_with_no_interaction(MDG, interaction_type_to_label, train_data)\n",
    "\n",
    "cluster_indices = [node_to_cluster[node] for node in MDG.nodes()]\n",
    "cluster_indices_tensor = torch.tensor(cluster_indices, dtype=torch.long)\n",
    "\n",
    "# Process validation data\n",
    "val_MDG = build_multidigraph_from_csv('cleaned_relations_val_final.csv')\n",
    "val_data = from_networkx(val_MDG)\n",
    "val_data.node_index = torch.arange(val_data.num_nodes)\n",
    "setup_edge_labels_with_no_interaction(val_MDG, interaction_type_to_label, val_data)\n",
    "val_node_to_cluster = map_nodes_to_clusters(val_MDG, gene_to_cluster)\n",
    "\n",
    "# Convert the node-cluster mapping to a tensor for the validation set\n",
    "val_cluster_indices = [val_node_to_cluster[node] for node in val_MDG.nodes()]\n",
    "val_cluster_indices_tensor = torch.tensor(val_cluster_indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "model = GraphTransformer(num_nodes, embedding_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dataset of edges and their labels\n",
    "edges = train_data.edge_index.t().tolist()  # List of [node_u, node_v]\n",
    "labels = [train_data.edge_label[edge[0], edge[1]].item() for edge in edges]\n",
    "\n",
    "edge_dataset = list(zip(edges, labels))\n",
    "edge_loader = DataLoader(edge_dataset, batch_size=1024, shuffle=True)  # Adjust batch_size as needed\n",
    "\n",
    "# Assuming val_data is structured similarly to train_data\n",
    "val_edges = val_data.edge_index.t().tolist()\n",
    "val_labels = [val_data.edge_label[edge[0], edge[1]].item() for edge in val_edges]\n",
    "\n",
    "val_edge_dataset = list(zip(val_edges, val_labels))\n",
    "val_edge_loader = DataLoader(val_edge_dataset, batch_size=1024, shuffle=False)  # You can adjust the batch size\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "inverse_interaction_type_to_label = {v: k for k, v in interaction_type_to_label.items()}\n",
    "\n",
    "def validate(val_data, val_cluster_indices_tensor, device, model, criterion):\n",
    "    model.eval()\n",
    "    total_cross_entropy_loss = 0  # Initialize cross-entropy loss\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_edge_loader, desc='Validating'):\n",
    "            edge_tensors, label_batch = batch\n",
    "            node_u_list, node_v_list = edge_tensors[0].to(device), edge_tensors[1].to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            # Pass both val_data and val_cluster_indices_tensor to the model\n",
    "            output = model(val_data.to(device), val_cluster_indices_tensor.to(device))\n",
    "            edge_predictions = output[node_u_list].argmax(dim=1)  # Predicted classes\n",
    "\n",
    "            # Calculate and accumulate cross-entropy loss\n",
    "            cross_entropy_loss = criterion(output[node_u_list], label_batch)\n",
    "            total_cross_entropy_loss += cross_entropy_loss.item()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            preds = edge_predictions.cpu().numpy()\n",
    "            true_labels = label_batch.cpu().numpy()\n",
    "            all_predictions.extend(preds)\n",
    "            all_true_labels.extend(true_labels)\n",
    "\n",
    "    # Calculate average cross-entropy loss over all batches\n",
    "    avg_cross_entropy_loss = total_cross_entropy_loss / len(val_edge_loader)\n",
    "\n",
    "    # Calculate F1 Score and classification report\n",
    "    weighted_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "    class_report = classification_report(all_true_labels, all_predictions, target_names=[inverse_interaction_type_to_label[i] for i in range(num_classes)], zero_division=0)\n",
    "\n",
    "    print(f\"Weighted F1 Score: {weighted_f1}\\nClassification Report:\\n{class_report}\")\n",
    "    print(f\"Validation Cross-Entropy Loss: {avg_cross_entropy_loss}\")\n",
    "\n",
    "    return weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = GraphTransformer(num_nodes, embedding_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Learning rate\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Loss function\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move your model to the chosen device\n",
    "model = model.to(device)\n",
    "\n",
    "def train(train_data, cluster_indices_tensor, model, optimizer, criterion, edge_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(edge_loader, desc='Training'):\n",
    "        edge_tensors, label_batch = batch\n",
    "        \n",
    "        node_u_list, node_v_list = edge_tensors[0].to(device), edge_tensors[1].to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass both train_data and cluster_indices_tensor to the model\n",
    "        output = model(train_data.to(device), cluster_indices_tensor.to(device))\n",
    "\n",
    "        edge_predictions = output[node_u_list]  # Corrected indexing\n",
    "        \n",
    "        loss = criterion(edge_predictions, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(edge_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [6:37:05<00:00, 238.25s/it]    \n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2827533016120433\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.08      0.04      0.05      1745\n",
      "         activation       0.43      0.29      0.34      7342\n",
      "binding/association       0.02      0.02      0.02       458\n",
      "        no_relation       0.34      0.50      0.40      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.01      0.04      0.02       694\n",
      "         expression       0.01      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.29     18128\n",
      "          macro avg       0.13      0.13      0.12     18128\n",
      "       weighted avg       0.30      0.29      0.28     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.82251458035575\n",
      "Epoch 1, Train Loss: 2.0498, Val F1 Score: 0.2828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.29298076201204165\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.05      0.01      0.02      1745\n",
      "         activation       0.43      0.30      0.35      7342\n",
      "binding/association       0.04      0.00      0.01       458\n",
      "        no_relation       0.34      0.63      0.44      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.01      0.01      0.01       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.34     18128\n",
      "          macro avg       0.12      0.14      0.12     18128\n",
      "       weighted avg       0.29      0.34      0.29     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7496245702107747\n",
      "Epoch 2, Train Loss: 1.8704, Val F1 Score: 0.2930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2972817815265219\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.04      0.01      0.01      1745\n",
      "         activation       0.45      0.29      0.35      7342\n",
      "binding/association       0.67      0.00      0.01       458\n",
      "        no_relation       0.34      0.69      0.45      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.01      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.35     18128\n",
      "          macro avg       0.22      0.14      0.12     18128\n",
      "       weighted avg       0.32      0.35      0.30     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.725456138451894\n",
      "Epoch 3, Train Loss: 1.7990, Val F1 Score: 0.2973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2930952420290469\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.16      0.01      0.01      1745\n",
      "         activation       0.46      0.26      0.33      7342\n",
      "binding/association       0.22      0.00      0.01       458\n",
      "        no_relation       0.34      0.73      0.46      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.36     18128\n",
      "          macro avg       0.17      0.14      0.12     18128\n",
      "       weighted avg       0.32      0.36      0.29     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7118615176942613\n",
      "Epoch 4, Train Loss: 1.7605, Val F1 Score: 0.2931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3052752798130321\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.16      0.01      0.01      1745\n",
      "         activation       0.48      0.30      0.37      7342\n",
      "binding/association       0.12      0.00      0.00       458\n",
      "        no_relation       0.34      0.72      0.46      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.36     18128\n",
      "          macro avg       0.16      0.15      0.12     18128\n",
      "       weighted avg       0.33      0.36      0.31     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7045549617873297\n",
      "Epoch 5, Train Loss: 1.7322, Val F1 Score: 0.3053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3044037158067715\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.23      0.00      0.00      1745\n",
      "         activation       0.48      0.30      0.37      7342\n",
      "binding/association       0.50      0.00      0.00       458\n",
      "        no_relation       0.34      0.72      0.46      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.36     18128\n",
      "          macro avg       0.22      0.15      0.12     18128\n",
      "       weighted avg       0.34      0.36      0.30     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6994346512688532\n",
      "Epoch 6, Train Loss: 1.7096, Val F1 Score: 0.3044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3093892014161965\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.49      0.31      0.38      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.72      0.46      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.37     18128\n",
      "          macro avg       0.12      0.15      0.12     18128\n",
      "       weighted avg       0.31      0.37      0.31     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6951010492112901\n",
      "Epoch 7, Train Loss: 1.6927, Val F1 Score: 0.3094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.31469521450157834\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.51      0.32      0.39      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.72      0.46      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.37     18128\n",
      "          macro avg       0.12      0.15      0.12     18128\n",
      "       weighted avg       0.32      0.37      0.31     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6895190874735515\n",
      "Epoch 8, Train Loss: 1.6801, Val F1 Score: 0.3147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3209837163922318\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.51      0.34      0.41      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.71      0.46      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.12      0.15      0.12     18128\n",
      "       weighted avg       0.32      0.38      0.32     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6854923632409837\n",
      "Epoch 9, Train Loss: 1.6660, Val F1 Score: 0.3210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:10<00:00,  1.41it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.32289175371741224\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.52      0.35      0.41      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.71      0.46      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.12      0.15      0.12     18128\n",
      "       weighted avg       0.32      0.38      0.32     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6822181873851352\n",
      "Epoch 10, Train Loss: 1.6569, Val F1 Score: 0.3229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3177307697486188\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.51      0.33      0.40      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.72      0.46      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.12      0.15      0.12     18128\n",
      "       weighted avg       0.32      0.38      0.32     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6801183422406514\n",
      "Epoch 11, Train Loss: 1.6435, Val F1 Score: 0.3177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.32816214170556357\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.51      0.37      0.43      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.70      0.45      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.03      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.39     18128\n",
      "          macro avg       0.13      0.15      0.13     18128\n",
      "       weighted avg       0.32      0.39      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6747062868542142\n",
      "Epoch 12, Train Loss: 1.6357, Val F1 Score: 0.3282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.33008397750638874\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.51      0.38      0.44      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.68      0.45      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.12      0.15      0.13     18128\n",
      "       weighted avg       0.32      0.38      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6723771890004475\n",
      "Epoch 13, Train Loss: 1.6287, Val F1 Score: 0.3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.33149406389794017\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.51      0.39      0.44      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.67      0.45      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.39     18128\n",
      "          macro avg       0.12      0.15      0.13     18128\n",
      "       weighted avg       0.32      0.39      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6678702235221863\n",
      "Epoch 14, Train Loss: 1.6180, Val F1 Score: 0.3315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3308804992804659\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.52      0.38      0.44      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.69      0.45      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.39     18128\n",
      "          macro avg       0.12      0.15      0.13     18128\n",
      "       weighted avg       0.32      0.39      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6637782255808513\n",
      "Epoch 15, Train Loss: 1.6131, Val F1 Score: 0.3309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3307857386990437\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.52      0.38      0.44      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.68      0.45      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.39     18128\n",
      "          macro avg       0.12      0.15      0.13     18128\n",
      "       weighted avg       0.32      0.39      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6606466836399503\n",
      "Epoch 16, Train Loss: 1.6037, Val F1 Score: 0.3308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3307718943729369\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.51      0.39      0.44      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.67      0.45      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.12      0.15      0.13     18128\n",
      "       weighted avg       0.32      0.38      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6563141081068251\n",
      "Epoch 17, Train Loss: 1.5987, Val F1 Score: 0.3308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3269570800838963\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.51      0.37      0.43      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.69      0.45      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.12      0.15      0.13     18128\n",
      "       weighted avg       0.32      0.38      0.33     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6567025714450412\n",
      "Epoch 18, Train Loss: 1.5912, Val F1 Score: 0.3270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.32356593058822075\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "         activation       0.51      0.35      0.42      7342\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "        no_relation       0.34      0.70      0.45      6143\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "           compound       0.00      0.00      0.00       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.12      0.15      0.12     18128\n",
      "       weighted avg       0.32      0.38      0.32     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6531583931710985\n",
      "Epoch 19, Train Loss: 1.5838, Val F1 Score: 0.3236\n",
      "Early stopping triggered\n",
      "Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Early stopping and scheduler parameters\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "best_f1_score = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Call train function with necessary parameters\n",
    "    train_loss = train(train_data, cluster_indices_tensor, model, optimizer, criterion, edge_loader, device)\n",
    "\n",
    "    # Call validate function with necessary parameters\n",
    "    val_f1_score = validate(val_data, val_cluster_indices_tensor, device, model, criterion)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val F1 Score: {val_f1_score:.4f}')\n",
    "\n",
    "    # Step the scheduler based on validation F1 score\n",
    "    scheduler.step(val_f1_score)\n",
    "\n",
    "    # Check for improvement in validation F1 score\n",
    "    if val_f1_score > best_f1_score:\n",
    "        best_f1_score = val_f1_score\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()  # Save the best model state\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Save the best model state\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'GTC_32_32_5_-3.pth')\n",
    "    print(\"Best model saved.\")\n",
    "else:\n",
    "    print(\"No model improvement was observed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_siamese_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
