{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '6'  # Adjust the number of threads as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def build_multidigraph_from_csv(csv_file):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            # Exclude 'no_relation' edges\n",
    "            if row['relation_type'] != 'no_relation':\n",
    "                # Add nodes with the 'name' attribute\n",
    "                G.add_node(row['starter_ID'], name=row['starter_ID'])\n",
    "                G.add_node(row['receiver_ID'], name=row['receiver_ID'])\n",
    "\n",
    "                # Add directed edges with additional attributes\n",
    "                weight = float(row['weight'])\n",
    "                G.add_edge(\n",
    "                    row['starter_ID'], \n",
    "                    row['receiver_ID'], \n",
    "                    weight=weight,\n",
    "                    interaction_type=row['subtype_name'],\n",
    "                    relation_type=row['relation_type'],\n",
    "                    pathway_sources=row['pathway_source'],\n",
    "                    credibility=row['credibility']\n",
    "                )\n",
    "    \n",
    "    return G\n",
    "\n",
    "def create_global_node_to_index_mapping(train_graph, val_graph):\n",
    "    all_nodes = set(train_graph.nodes()).union(set(val_graph.nodes()))\n",
    "    return {node: i for i, node in enumerate(all_nodes)}\n",
    "\n",
    "# Paths to the CSV files\n",
    "train_csv_path = 'relations_train_final.csv'\n",
    "val_csv_path = 'cleaned_relations_val_final.csv'\n",
    "# Build the MultiDiGraphs\n",
    "train_MDG = build_multidigraph_from_csv(train_csv_path)\n",
    "val_MDG = build_multidigraph_from_csv(val_csv_path)\n",
    "\n",
    "# Create a global node to index mapping\n",
    "global_node_to_index = create_global_node_to_index_mapping(train_MDG, val_MDG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "def apply_mapping_and_get_indices(graph, mapping):\n",
    "    # Create a tensor of node indices based on the global mapping\n",
    "    num_nodes = len(mapping)\n",
    "    node_indices = torch.arange(num_nodes)\n",
    "\n",
    "    # Remap nodes in the graph according to the global mapping\n",
    "    remapped_graph = nx.relabel_nodes(graph, mapping)\n",
    "\n",
    "    return remapped_graph, node_indices\n",
    "\n",
    "# Apply mapping to training and validation graphs\n",
    "remapped_train_MDG, train_indices = apply_mapping_and_get_indices(train_MDG, global_node_to_index)\n",
    "remapped_val_MDG, val_indices = apply_mapping_and_get_indices(val_MDG, global_node_to_index)\n",
    "\n",
    "# Convert to PyTorch Geometric Data\n",
    "train_data = from_networkx(remapped_train_MDG)\n",
    "train_data.x = train_indices\n",
    "\n",
    "val_data = from_networkx(remapped_val_MDG)\n",
    "val_data.x = val_indices\n",
    "\n",
    "train_data.x = train_data.x.long()  # Convert to LongTensor\n",
    "val_data.x = val_data.x.long()  # Convert to LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "from torch_geometric.nn.models import VGAE\n",
    "\n",
    "class GATGCNVarEncoder(torch.nn.Module):\n",
    "    def __init__(self, max_nodes, embedding_dim, out_channels):\n",
    "        super(GATGCNVarEncoder, self).__init__()\n",
    "        self.node_emb = torch.nn.Embedding(max_nodes, embedding_dim)\n",
    "        self.conv1 = GCNConv(embedding_dim, 2 * out_channels)\n",
    "\n",
    "        # Two parallel layers for mean and log-variance\n",
    "        self.conv_mu = GATConv(2 * out_channels, out_channels, heads=1, dropout=0.2)\n",
    "        self.conv_logvar = GATConv(2 * out_channels, out_channels, heads=1, dropout=0.2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.node_emb(x)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        # Separate computation for mean and log-variance\n",
    "        mu = self.conv_mu(x, edge_index)\n",
    "        logvar = self.conv_logvar(x, edge_index)\n",
    "        return mu, logvar\n",
    "\n",
    "# Usage\n",
    "max_nodes = 5000  # Set to a number higher than your expected number of nodes\n",
    "embedding_dim = 16\n",
    "out_channels = 16\n",
    "\n",
    "var_encoder = GATGCNVarEncoder(max_nodes, embedding_dim, out_channels)\n",
    "# Create the VGAE model\n",
    "model = VGAE(var_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "\n",
    "def validate_vgae(model, val_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode the validation data\n",
    "        z_val = model.encode(val_data.x, val_data.edge_index)\n",
    "\n",
    "        # Access mu and logstd directly\n",
    "        mu_val, logstd_val = model.__mu__, model.__logstd__\n",
    "\n",
    "        # Calculate the reconstruction loss\n",
    "        val_recon_loss = model.recon_loss(z_val, val_data.edge_index)\n",
    "\n",
    "        # Calculate the KL divergence\n",
    "        val_kl_loss = model.kl_loss(mu_val, logstd_val)\n",
    "\n",
    "        # Combine the losses\n",
    "        val_loss = val_recon_loss + (1 / val_data.num_nodes) * val_kl_loss\n",
    "\n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "def broad_search(embeddings, step, max_clusters):\n",
    "    best_score = -1\n",
    "    best_n_clusters = 0\n",
    "    for n_clusters in range(2, max_clusters + 1, step):\n",
    "        score = calculate_silhouette_score(embeddings, n_clusters)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n_clusters = n_clusters\n",
    "    return best_score, best_n_clusters\n",
    "\n",
    "def detailed_search(embeddings, start, end, step):\n",
    "    best_score = -1\n",
    "    best_n_clusters = 0\n",
    "    for n_clusters in range(start, end + 1, step):\n",
    "        score = calculate_silhouette_score(embeddings, n_clusters)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n_clusters = n_clusters\n",
    "    return best_score, best_n_clusters\n",
    "\n",
    "def calculate_silhouette_score(embeddings, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    return silhouette_score(embeddings, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_vgae(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Encode the training data\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "\n",
    "    # Access mu and logstd directly\n",
    "    mu, logstd = model.__mu__, model.__logstd__\n",
    "\n",
    "    # Calculate the reconstruction loss\n",
    "    recon_loss = model.recon_loss(z, data.edge_index)\n",
    "\n",
    "    # Calculate the KL divergence\n",
    "    kl_loss = model.kl_loss(mu, logstd)\n",
    "\n",
    "    # Combine the losses\n",
    "    total_loss = recon_loss + (1 / data.num_nodes) * kl_loss\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return total_loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 5.2999, Val Loss: 2.4055\n",
      "Epoch: 2, Loss: 4.1038, Val Loss: 1.9676\n",
      "Epoch: 3, Loss: 3.3534, Val Loss: 1.7506\n",
      "Epoch: 4, Loss: 2.9585, Val Loss: 1.6269\n",
      "Epoch: 5, Loss: 2.5971, Val Loss: 1.5697\n",
      "Epoch: 6, Loss: 2.4067, Val Loss: 1.5289\n",
      "Epoch: 7, Loss: 2.1629, Val Loss: 1.4990\n",
      "Epoch: 8, Loss: 2.0050, Val Loss: 1.4561\n",
      "Epoch: 9, Loss: 1.8515, Val Loss: 1.4594\n",
      "Epoch: 10, Loss: 1.7590, Val Loss: 1.4412\n",
      "Epoch: 11, Loss: 1.7214, Val Loss: 1.4480\n",
      "Epoch: 12, Loss: 1.6187, Val Loss: 1.4449\n",
      "Epoch: 13, Loss: 1.5593, Val Loss: 1.4341\n",
      "Epoch: 14, Loss: 1.5325, Val Loss: 1.4365\n",
      "Epoch: 15, Loss: 1.4885, Val Loss: 1.4298\n",
      "Epoch: 16, Loss: 1.4440, Val Loss: 1.4106\n",
      "Epoch: 17, Loss: 1.4118, Val Loss: 1.4038\n",
      "Epoch: 18, Loss: 1.3883, Val Loss: 1.3889\n",
      "Epoch: 19, Loss: 1.3499, Val Loss: 1.3879\n",
      "Epoch: 20, Loss: 1.3402, Val Loss: 1.3560\n",
      "Epoch: 21, Loss: 1.3081, Val Loss: 1.3540\n",
      "Epoch: 22, Loss: 1.2857, Val Loss: 1.3475\n",
      "Epoch: 23, Loss: 1.2840, Val Loss: 1.3500\n",
      "Epoch: 24, Loss: 1.2793, Val Loss: 1.3476\n",
      "Epoch: 25, Loss: 1.2680, Val Loss: 1.3510\n",
      "Epoch: 26, Loss: 1.2536, Val Loss: 1.3441\n",
      "Epoch: 27, Loss: 1.2517, Val Loss: 1.3481\n",
      "Epoch: 28, Loss: 1.2286, Val Loss: 1.3408\n",
      "Epoch: 29, Loss: 1.2311, Val Loss: 1.3385\n",
      "Epoch: 30, Loss: 1.2178, Val Loss: 1.3395\n",
      "Epoch: 31, Loss: 1.2181, Val Loss: 1.3354\n",
      "Epoch: 32, Loss: 1.2001, Val Loss: 1.3244\n",
      "Epoch: 33, Loss: 1.1938, Val Loss: 1.3306\n",
      "Epoch: 34, Loss: 1.1924, Val Loss: 1.3350\n",
      "Epoch: 35, Loss: 1.1784, Val Loss: 1.3477\n",
      "Epoch: 36, Loss: 1.1726, Val Loss: 1.3540\n",
      "Epoch: 37, Loss: 1.1698, Val Loss: 1.3371\n",
      "Epoch: 38, Loss: 1.1645, Val Loss: 1.3359\n",
      "Epoch: 39, Loss: 1.1608, Val Loss: 1.3278\n",
      "Epoch: 40, Loss: 1.1586, Val Loss: 1.3279\n",
      "Epoch: 41, Loss: 1.1490, Val Loss: 1.3224\n",
      "Epoch: 42, Loss: 1.1593, Val Loss: 1.3098\n",
      "Epoch: 43, Loss: 1.1585, Val Loss: 1.3148\n",
      "Epoch: 44, Loss: 1.1462, Val Loss: 1.3081\n",
      "Epoch: 45, Loss: 1.1457, Val Loss: 1.3167\n",
      "Epoch: 46, Loss: 1.1356, Val Loss: 1.3250\n",
      "Epoch: 47, Loss: 1.1377, Val Loss: 1.3243\n",
      "Epoch: 48, Loss: 1.1342, Val Loss: 1.3293\n",
      "Epoch: 49, Loss: 1.1335, Val Loss: 1.3189\n",
      "Epoch: 50, Loss: 1.1320, Val Loss: 1.3045\n",
      "Epoch: 51, Loss: 1.1302, Val Loss: 1.3031\n",
      "Epoch: 52, Loss: 1.1283, Val Loss: 1.2979\n",
      "Epoch: 53, Loss: 1.1286, Val Loss: 1.3066\n",
      "Epoch: 54, Loss: 1.1273, Val Loss: 1.3063\n",
      "Epoch: 55, Loss: 1.1219, Val Loss: 1.3122\n",
      "Epoch: 56, Loss: 1.1248, Val Loss: 1.3111\n",
      "Epoch: 57, Loss: 1.1155, Val Loss: 1.3059\n",
      "Epoch: 58, Loss: 1.1120, Val Loss: 1.2917\n",
      "Epoch: 59, Loss: 1.1059, Val Loss: 1.2801\n",
      "Epoch: 60, Loss: 1.1039, Val Loss: 1.2935\n",
      "Epoch: 61, Loss: 1.1057, Val Loss: 1.2935\n",
      "Epoch: 62, Loss: 1.1044, Val Loss: 1.2890\n",
      "Epoch: 63, Loss: 1.1031, Val Loss: 1.3064\n",
      "Epoch: 64, Loss: 1.1071, Val Loss: 1.3080\n",
      "Epoch: 65, Loss: 1.1031, Val Loss: 1.3126\n",
      "Epoch: 66, Loss: 1.0968, Val Loss: 1.3145\n",
      "Epoch: 67, Loss: 1.0935, Val Loss: 1.3110\n",
      "Epoch: 68, Loss: 1.1019, Val Loss: 1.2992\n",
      "Epoch: 69, Loss: 1.0976, Val Loss: 1.2992\n",
      "Early stopping triggered\n",
      "Best model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Early stopping and model saving parameters\n",
    "patience = 10\n",
    "best_val_score = -1\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "best_model_state = None  # To store the best model state\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train_vgae(model, train_data, optimizer)  # Updated train function call\n",
    "    val_loss = validate_vgae(model, val_data)  # Updated validate function call\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    scheduler.step(val_loss)  # Update based on val_loss\n",
    "\n",
    "    # Early stopping based on the validation loss\n",
    "    if val_loss < best_val_score or best_val_score == -1:\n",
    "        best_val_score = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'best_vgae_model.pth')  # Update the file name\n",
    "    print(\"Best model saved.\")\n",
    "else:\n",
    "    print(\"No model improvement was observed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the trained model and generate embeddings\n",
    "model.load_state_dict(torch.load('vgae_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    embeddings = z.cpu().numpy()\n",
    "\n",
    "# Step 2: Apply Spectral Clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "n_clusters = 92\n",
    "spectral_clustering = SpectralClustering(n_clusters=n_clusters, random_state=42, affinity='nearest_neighbors')\n",
    "cluster_labels = spectral_clustering.fit_predict(embeddings)\n",
    "\n",
    "# Step 3: Map Clusters to Gene Names\n",
    "index_to_gene = {index: gene for gene, index in global_node_to_index.items()}\n",
    "gene_names = [index_to_gene[i] for i in range(len(embeddings))]\n",
    "gene_cluster_pairs = list(zip(gene_names, cluster_labels))\n",
    "\n",
    "# Step 4: Write Results to CSV\n",
    "import csv\n",
    "\n",
    "with open('vgae_spectral_gene_cluster_assignments.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Gene', 'Cluster'])\n",
    "    for gene, cluster in gene_cluster_pairs:\n",
    "        writer.writerow([gene, cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Assuming the rest of your setup is the same\n",
    "\n",
    "# Step 2: Apply KMeans Clustering instead of Spectral Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 92  # The number of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Mapping Clusters to Gene Names\n",
    "index_to_gene = {index: gene for gene, index in global_node_to_index.items()}\n",
    "gene_names = [index_to_gene[i] for i in range(len(embeddings))]\n",
    "gene_cluster_pairs = list(zip(gene_names, cluster_labels))\n",
    "\n",
    "# Write Results to CSV\n",
    "import csv\n",
    "\n",
    "with open('vgae_kmeans_gene_cluster_assignments.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Gene', 'Cluster'])\n",
    "    for gene, cluster in gene_cluster_pairs:\n",
    "        writer.writerow([gene, cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfuzzy.cluster import cmeans\n",
    "\n",
    "# Convert embeddings to the required format for FCM\n",
    "embeddings_T = np.transpose(embeddings)  # Transpose embeddings for cmeans\n",
    "\n",
    "# Apply Fuzzy C-Means\n",
    "cntr, u, _, _, _, _, _ = cmeans(embeddings_T, n_clusters, 2, error=0.005, maxiter=1000, init=None)\n",
    "\n",
    "# u contains the membership matrix\n",
    "# Transpose the matrix so each row corresponds to a gene\n",
    "membership_matrix = np.transpose(u)\n",
    "\n",
    "# Writing Fuzzy Clustering Results to CSV\n",
    "with open('vgae_fcm_gene_cluster_assignments.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['Gene'] + [f'Cluster_{i}' for i in range(n_clusters)]\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for gene_name, memberships in zip(gene_names, membership_matrix):\n",
    "        writer.writerow([gene_name] + list(memberships))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_siamese_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
