{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4816\n",
      "Number of edges: 101782\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "def build_multidigraph_from_csv(csv_file):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            # Add nodes (if not already added)\n",
    "            G.add_node(row['starter_ID'], name=row['starter_ID'])\n",
    "            G.add_node(row['receiver_ID'], name=row['receiver_ID'])\n",
    "\n",
    "            # Add directed edges with additional attributes\n",
    "            # Each edge is unique and can represent a different type of interaction\n",
    "            G.add_edge(\n",
    "                row['starter_ID'], \n",
    "                row['receiver_ID'], \n",
    "                interaction_type=row['subtype_name'],\n",
    "                relation_type=row['relation_type'],\n",
    "                pathway_sources=row['pathway_source'],\n",
    "                credibility=row['credibility']\n",
    "            )\n",
    "\n",
    "    return G\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'relations_train_final.csv'  # Update this to the path of your relations_train.csv file\n",
    "\n",
    "# Build the multidigraph\n",
    "MDG = build_multidigraph_from_csv(csv_file_path)\n",
    "\n",
    "# Print basic information about the multidigraph\n",
    "print(f\"Number of nodes: {MDG.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {MDG.number_of_edges()}\")\n",
    "\n",
    "# The multidigraph `MDG` is now constructed with nodes and multiple types of directed edges from the relations_train.csv\n",
    "# You can now use `MDG` for further analysis or as input to clustering algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_cluster_assignments(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    gene_to_cluster = {row['Gene']: row['Cluster'] for _, row in df.iterrows()}\n",
    "    return gene_to_cluster\n",
    "\n",
    "# Use this function to read your CSV file\n",
    "gene_to_cluster = read_cluster_assignments('vgae_spectral_gene_cluster_assignments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "\n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, num_classes, num_hidden_units=32, num_heads=5):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.node_emb = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "        \n",
    "        # First Graph Transformer layer\n",
    "        self.conv1 = TransformerConv(embedding_dim, num_hidden_units, heads=num_heads, dropout=0.6, edge_dim=None)\n",
    "        \n",
    "        # Output layer\n",
    "        self.conv2 = TransformerConv(num_hidden_units * num_heads, num_classes, heads=1, concat=True, dropout=0.6, edge_dim=None)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.node_emb(data.node_index)\n",
    "        edge_index = data.edge_index\n",
    "        edge_weight = None  # If you have edge weights, they should be used here\n",
    "\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Assuming MDG is your MultiDiGraph\n",
    "train_data = from_networkx(MDG)\n",
    "train_data.node_index = torch.arange(train_data.num_nodes)  # Node indexing\n",
    "\n",
    "num_nodes = train_data.num_nodes\n",
    "embedding_dim = 32  # Choose an appropriate embedding size\n",
    "num_classes = 7   # Set the number of classes based on your edge types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all unique interaction types from MDG\n",
    "interaction_types = set()\n",
    "for _, _, edge_data in MDG.edges(data=True):\n",
    "    interaction_types.add(edge_data['interaction_type'])\n",
    "\n",
    "# Update the mapping to include 'no interaction' class\n",
    "interaction_type_to_label = {inter_type: i for i, inter_type in enumerate(interaction_types)}\n",
    "num_classes = len(interaction_type_to_label)\n",
    "\n",
    "def setup_edge_labels_with_no_interaction(MDG, interaction_type_to_label, data):\n",
    "    num_nodes = len(MDG.nodes())\n",
    "    edge_labels = torch.zeros((num_nodes, num_nodes), dtype=torch.long)\n",
    "\n",
    "    for u, v, edge_data in MDG.edges(data=True):\n",
    "        u_index = list(MDG.nodes()).index(u)\n",
    "        v_index = list(MDG.nodes()).index(v)\n",
    "        label = interaction_type_to_label[edge_data['interaction_type']]\n",
    "        edge_labels[u_index, v_index] = label\n",
    "\n",
    "    data.edge_label = edge_labels  # Directly modify the data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Process training data\n",
    "train_MDG = build_multidigraph_from_csv('relations_train_final.csv')\n",
    "train_data = from_networkx(train_MDG)\n",
    "train_data.node_index = torch.arange(train_data.num_nodes)\n",
    "setup_edge_labels_with_no_interaction(train_MDG, interaction_type_to_label, train_data)\n",
    "\n",
    "# Process testidation data\n",
    "val_MDG = build_multidigraph_from_csv('cleaned_relations_val_final.csv')\n",
    "val_data = from_networkx(val_MDG)\n",
    "val_data.node_index = torch.arange(val_data.num_nodes)\n",
    "setup_edge_labels_with_no_interaction(val_MDG, interaction_type_to_label, val_data)\n",
    "\n",
    "model = GraphTransformer(num_nodes, embedding_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MDG = build_multidigraph_from_csv('cleaned_relations_test_final.csv')\n",
    "test_data = from_networkx(test_MDG)\n",
    "test_data.node_index = torch.arange(test_data.num_nodes)\n",
    "setup_edge_labels_with_no_interaction(test_MDG, interaction_type_to_label, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dataset of edges and their labels\n",
    "edges = train_data.edge_index.t().tolist()  # List of [node_u, node_v]\n",
    "labels = [train_data.edge_label[edge[0], edge[1]].item() for edge in edges]\n",
    "\n",
    "edge_dataset = list(zip(edges, labels))\n",
    "edge_loader = DataLoader(edge_dataset, batch_size=1024, shuffle=True)  # Adjust batch_size as needed\n",
    "\n",
    "# Assuming val_data is structured similarly to train_data\n",
    "val_edges = val_data.edge_index.t().tolist()\n",
    "val_labels = [val_data.edge_label[edge[0], edge[1]].item() for edge in val_edges]\n",
    "\n",
    "val_edge_dataset = list(zip(val_edges, val_labels))\n",
    "val_edge_loader = DataLoader(val_edge_dataset, batch_size=1024, shuffle=False)  # You can adjust the batch size\n",
    "\n",
    "test_edges = test_data.edge_index.t().tolist()\n",
    "test_labels = [test_data.edge_label[edge[0], edge[1]].item() for edge in test_edges]\n",
    "\n",
    "test_edge_dataset = list(zip(test_edges, test_labels))\n",
    "test_edge_loader = DataLoader(test_edge_dataset, batch_size=1024, shuffle=False)  # You can adjust the batch size\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "inverse_interaction_type_to_label = {v: k for k, v in interaction_type_to_label.items()}\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_cross_entropy_loss = 0  # Initialize cross-entropy loss\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_edge_loader, desc='Validating'):\n",
    "            edge_tensors, label_batch = batch\n",
    "            node_u_list, node_v_list = edge_tensors[0].to(device), edge_tensors[1].to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            output = model(val_data.to(device))\n",
    "            edge_predictions = output[node_u_list].argmax(dim=1)  # Predicted classes\n",
    "\n",
    "            # Calculate and accumulate cross-entropy loss\n",
    "            cross_entropy_loss = criterion(output[node_u_list], label_batch)\n",
    "            total_cross_entropy_loss += cross_entropy_loss.item()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            preds = edge_predictions.cpu().numpy()\n",
    "            true_labels = label_batch.cpu().numpy()\n",
    "            all_predictions.extend(preds)\n",
    "            all_true_labels.extend(true_labels)\n",
    "\n",
    "    # Calculate average cross-entropy loss over all batches\n",
    "    avg_cross_entropy_loss = total_cross_entropy_loss / len(val_edge_loader)\n",
    "\n",
    "    # Calculate F1 Score and classification report\n",
    "    weighted_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "    class_report = classification_report(all_true_labels, all_predictions, target_names=[inverse_interaction_type_to_label[i] for i in range(num_classes)], zero_division=0)\n",
    "\n",
    "    print(f\"Weighted F1 Score: {weighted_f1}\\nClassification Report:\\n{class_report}\")\n",
    "    print(f\"Validation Cross-Entropy Loss: {avg_cross_entropy_loss}\")\n",
    "\n",
    "    return weighted_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = GraphTransformer(num_nodes, embedding_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Learning rate\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Loss function\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move your model to the chosen device\n",
    "model = model.to(device)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(edge_loader, desc='Training'):\n",
    "        edge_tensors, label_batch = batch\n",
    "        \n",
    "        node_u_list, node_v_list = edge_tensors[0].to(device), edge_tensors[1].to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_data.to(device))\n",
    "\n",
    "        edge_predictions = output[node_u_list]  # Corrected indexing\n",
    "        \n",
    "        loss = criterion(edge_predictions, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(edge_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.24081056742507587\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        no_relation       0.34      0.44      0.38      6143\n",
      "         expression       0.01      0.00      0.01       722\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "           compound       0.03      0.21      0.05       694\n",
      "binding/association       0.02      0.01      0.01       458\n",
      "         activation       0.35      0.22      0.27      7342\n",
      "\n",
      "           accuracy                           0.25     18128\n",
      "          macro avg       0.11      0.13      0.10     18128\n",
      "       weighted avg       0.26      0.25      0.24     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.8602794607480366\n",
      "Epoch 1, Train Loss: 2.0141, Val F1 Score: 0.2408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2394147642278179\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        no_relation       0.34      0.58      0.43      6143\n",
      "         expression       0.00      0.00      0.00       722\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "           compound       0.02      0.11      0.04       694\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         activation       0.33      0.18      0.23      7342\n",
      "\n",
      "           accuracy                           0.27     18128\n",
      "          macro avg       0.10      0.12      0.10     18128\n",
      "       weighted avg       0.25      0.27      0.24     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7992704576916165\n",
      "Epoch 2, Train Loss: 1.8647, Val F1 Score: 0.2394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:09<00:00,  1.43it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.25689393428596985\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        no_relation       0.34      0.67      0.45      6143\n",
      "         expression       0.00      0.00      0.00       722\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "           compound       0.02      0.06      0.03       694\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         activation       0.36      0.20      0.25      7342\n",
      "\n",
      "           accuracy                           0.31     18128\n",
      "          macro avg       0.10      0.13      0.11     18128\n",
      "       weighted avg       0.26      0.31      0.26     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7694940699471369\n",
      "Epoch 3, Train Loss: 1.8037, Val F1 Score: 0.2569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:10<00:00,  1.41it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.25122254656789195\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        no_relation       0.34      0.70      0.46      6143\n",
      "         expression       0.00      0.00      0.00       722\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "           compound       0.02      0.05      0.03       694\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         activation       0.35      0.18      0.23      7342\n",
      "\n",
      "           accuracy                           0.31     18128\n",
      "          macro avg       0.10      0.13      0.10     18128\n",
      "       weighted avg       0.26      0.31      0.25     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7569267021285162\n",
      "Epoch 4, Train Loss: 1.7608, Val F1 Score: 0.2512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.24801680817634586\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        no_relation       0.34      0.76      0.47      6143\n",
      "         expression       0.00      0.00      0.00       722\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "           compound       0.03      0.05      0.03       694\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         activation       0.36      0.15      0.22      7342\n",
      "\n",
      "           accuracy                           0.32     18128\n",
      "          macro avg       0.10      0.14      0.10     18128\n",
      "       weighted avg       0.26      0.32      0.25     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7473703490363226\n",
      "Epoch 5, Train Loss: 1.7320, Val F1 Score: 0.2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.24603719627419549\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        no_relation       0.34      0.79      0.47      6143\n",
      "         expression       0.00      0.00      0.00       722\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "           compound       0.02      0.03      0.02       694\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         activation       0.37      0.14      0.21      7342\n",
      "\n",
      "           accuracy                           0.33     18128\n",
      "          macro avg       0.10      0.14      0.10     18128\n",
      "       weighted avg       0.27      0.33      0.25     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7394248247146606\n",
      "Epoch 6, Train Loss: 1.7109, Val F1 Score: 0.2460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:10<00:00,  1.43it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.24642270263564023\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        no_relation       0.34      0.80      0.48      6143\n",
      "         expression       0.00      0.00      0.00       722\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "           compound       0.01      0.01      0.01       694\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         activation       0.37      0.14      0.21      7342\n",
      "\n",
      "           accuracy                           0.33     18128\n",
      "          macro avg       0.10      0.14      0.10     18128\n",
      "       weighted avg       0.27      0.33      0.25     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7327967882156372\n",
      "Epoch 7, Train Loss: 1.6962, Val F1 Score: 0.2464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2496369349263342\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "        no_relation       0.34      0.83      0.48      6143\n",
      "         expression       0.00      0.00      0.00       722\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "           compound       0.01      0.01      0.01       694\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "         activation       0.40      0.14      0.21      7342\n",
      "\n",
      "           accuracy                           0.34     18128\n",
      "          macro avg       0.11      0.14      0.10     18128\n",
      "       weighted avg       0.28      0.34      0.25     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7293875416119893\n",
      "Epoch 8, Train Loss: 1.6803, Val F1 Score: 0.2496\n",
      "Early stopping triggered\n",
      "Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs to wait for improvement before stopping\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "best_f1_score = 0\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(1000):\n",
    "    train_loss = train()  # Your training function\n",
    "    val_f1_score = validate()  # Your validation function returning F1 score\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val F1 Score: {val_f1_score:.4f}')\n",
    "\n",
    "    # Step the scheduler with the F1 score\n",
    "    scheduler.step(val_f1_score)\n",
    "\n",
    "    # Check for improvement\n",
    "    if val_f1_score > best_f1_score:\n",
    "        best_f1_score = val_f1_score\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()  # Update the best model state\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Save the best model state (outside of the loop)\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'GT_32_32_5_-3.pth')\n",
    "    print(\"Best model saved.\")\n",
    "else:\n",
    "    print(\"No model improvement was observed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_siamese_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
