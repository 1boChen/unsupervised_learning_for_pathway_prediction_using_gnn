{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4816\n",
      "Number of edges: 101782\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "def build_multidigraph_from_csv(csv_file):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            # Add nodes (if not already added)\n",
    "            G.add_node(row['starter_ID'], name=row['starter_ID'])\n",
    "            G.add_node(row['receiver_ID'], name=row['receiver_ID'])\n",
    "\n",
    "            # Add directed edges with additional attributes\n",
    "            # Each edge is unique and can represent a different type of interaction\n",
    "            G.add_edge(\n",
    "                row['starter_ID'], \n",
    "                row['receiver_ID'], \n",
    "                interaction_type=row['subtype_name'],\n",
    "                relation_type=row['relation_type'],\n",
    "                pathway_sources=row['pathway_source'],\n",
    "                credibility=row['credibility']\n",
    "            )\n",
    "\n",
    "    return G\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'relations_train_final.csv'  # Update this to the path of your relations_train.csv file\n",
    "\n",
    "# Build the multidigraph\n",
    "MDG = build_multidigraph_from_csv(csv_file_path)\n",
    "\n",
    "# Print basic information about the multidigraph\n",
    "print(f\"Number of nodes: {MDG.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {MDG.number_of_edges()}\")\n",
    "\n",
    "# The multidigraph `MDG` is now constructed with nodes and multiple types of directed edges from the relations_train.csv\n",
    "# You can now use `MDG` for further analysis or as input to clustering algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "\n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, num_classes, cluster_feature_dim=92, num_hidden_units=32, num_heads=5):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.node_emb = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "\n",
    "        # First Graph Transformer layer\n",
    "        self.conv1 = TransformerConv(embedding_dim + cluster_feature_dim, num_hidden_units, heads=num_heads, dropout=0.6, edge_dim=None)\n",
    "        \n",
    "        # Output layer\n",
    "        self.conv2 = TransformerConv(num_hidden_units * num_heads, num_classes, heads=1, concat=True, dropout=0.6, edge_dim=None)\n",
    "\n",
    "    def forward(self, data, cluster_features):\n",
    "        x = self.node_emb(data.node_index)\n",
    "        # cluster_features is a [num_nodes x 92] tensor containing the fuzzy membership scores\n",
    "        x = torch.cat([x, cluster_features], dim=1)  # Concatenate node and cluster features\n",
    "\n",
    "        edge_index = data.edge_index\n",
    "        edge_weight = None  # Update if you have edge weights\n",
    "\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Assuming MDG is your MultiDiGraph\n",
    "train_data = from_networkx(MDG)\n",
    "\n",
    "num_nodes = train_data.num_nodes\n",
    "embedding_dim = 32  # Choose an appropriate embedding size\n",
    "num_classes = 7   # Set the number of classes based on your edge types\n",
    "num_clusters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all unique interaction types from MDG\n",
    "interaction_types = set()\n",
    "for _, _, edge_data in MDG.edges(data=True):\n",
    "    interaction_types.add(edge_data['interaction_type'])\n",
    "\n",
    "# Update the mapping to include 'no interaction' class\n",
    "interaction_type_to_label = {inter_type: i for i, inter_type in enumerate(interaction_types)}\n",
    "num_classes = len(interaction_type_to_label)\n",
    "\n",
    "def setup_edge_labels_with_no_interaction(MDG, interaction_type_to_label, data):\n",
    "    num_nodes = len(MDG.nodes())\n",
    "    edge_labels = torch.zeros((num_nodes, num_nodes), dtype=torch.long)\n",
    "\n",
    "    for u, v, edge_data in MDG.edges(data=True):\n",
    "        u_index = list(MDG.nodes()).index(u)\n",
    "        v_index = list(MDG.nodes()).index(v)\n",
    "        label = interaction_type_to_label[edge_data['interaction_type']]\n",
    "        edge_labels[u_index, v_index] = label\n",
    "\n",
    "    data.edge_label = edge_labels  # Directly modify the data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_cluster_assignments(csv_file):\n",
    "    return pd.read_csv(csv_file).set_index('Gene')\n",
    "\n",
    "# Use this function to read your CSV file\n",
    "cluster_df = read_cluster_assignments('vgae_fcm_gene_cluster_assignments.csv')\n",
    "\n",
    "def map_nodes_to_cluster_features(G, cluster_df):\n",
    "    cluster_features = []\n",
    "    for node in G.nodes():\n",
    "        # Default to a vector of zeros if the gene is not in the DataFrame\n",
    "        features = cluster_df.loc[node].values if node in cluster_df.index else np.zeros(cluster_df.shape[1])\n",
    "        cluster_features.append(features)\n",
    "    return torch.tensor(cluster_features, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ycy6y\\AppData\\Local\\Temp\\ipykernel_21260\\2455542463.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  return torch.tensor(cluster_features, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Function to map nodes in a graph to global indices\n",
    "def map_nodes_to_global_indices(G, global_node_to_index):\n",
    "    return [global_node_to_index[node] for node in G.nodes()]\n",
    "\n",
    "\n",
    "# Process validation data\n",
    "val_MDG = build_multidigraph_from_csv('cleaned_relations_val_final.csv')\n",
    "val_data = from_networkx(val_MDG)\n",
    "\n",
    "# Create a global node to index mapping\n",
    "all_nodes = set(MDG.nodes()).union(set(val_MDG.nodes()))\n",
    "global_node_to_index = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "\n",
    "\n",
    "setup_edge_labels_with_no_interaction(MDG, interaction_type_to_label, train_data)\n",
    "train_data.node_index = torch.tensor(map_nodes_to_global_indices(MDG, global_node_to_index), dtype=torch.long)\n",
    "val_data.node_index = torch.tensor(map_nodes_to_global_indices(val_MDG, global_node_to_index), dtype=torch.long)\n",
    "\n",
    "# Now proceed with the rest of your data preparation\n",
    "train_cluster_features = map_nodes_to_cluster_features(MDG, cluster_df)\n",
    "val_cluster_features = map_nodes_to_cluster_features(val_MDG, cluster_df)\n",
    "setup_edge_labels_with_no_interaction(val_MDG, interaction_type_to_label, val_data)\n",
    "\n",
    "model = GraphTransformer(num_nodes, embedding_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dataset of edges and their labels\n",
    "edges = train_data.edge_index.t().tolist()  # List of [node_u, node_v]\n",
    "labels = [train_data.edge_label[edge[0], edge[1]].item() for edge in edges]\n",
    "\n",
    "edge_dataset = list(zip(edges, labels))\n",
    "edge_loader = DataLoader(edge_dataset, batch_size=1024, shuffle=True)  # Adjust batch_size as needed\n",
    "\n",
    "# Assuming val_data is structured similarly to train_data\n",
    "val_edges = val_data.edge_index.t().tolist()\n",
    "val_labels = [val_data.edge_label[edge[0], edge[1]].item() for edge in val_edges]\n",
    "\n",
    "val_edge_dataset = list(zip(val_edges, val_labels))\n",
    "val_edge_loader = DataLoader(val_edge_dataset, batch_size=1024, shuffle=False)  # You can adjust the batch size\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "inverse_interaction_type_to_label = {v: k for k, v in interaction_type_to_label.items()}\n",
    "\n",
    "def validate(model, val_data, val_cluster_features, val_edge_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_cross_entropy_loss = 0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_edge_loader, desc='Validating'):\n",
    "            edge_tensors, label_batch = batch\n",
    "\n",
    "            node_u_list, node_v_list = edge_tensors[0].to(device), edge_tensors[1].to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            # Pass both val_data and val_cluster_features to the model\n",
    "            output = model(val_data.to(device), val_cluster_features.to(device))\n",
    "            edge_predictions = output[node_u_list].argmax(dim=1)\n",
    "\n",
    "            cross_entropy_loss = criterion(output[node_u_list], label_batch)\n",
    "            total_cross_entropy_loss += cross_entropy_loss.item()\n",
    "\n",
    "            preds = edge_predictions.cpu().numpy()\n",
    "            true_labels = label_batch.cpu().numpy()\n",
    "            all_predictions.extend(preds)\n",
    "            all_true_labels.extend(true_labels)\n",
    "\n",
    "    avg_cross_entropy_loss = total_cross_entropy_loss / len(val_edge_loader)\n",
    "    weighted_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "    class_report = classification_report(all_true_labels, all_predictions, target_names=[inverse_interaction_type_to_label[i] for i in range(num_classes)], zero_division=0)\n",
    "\n",
    "    print(f\"Weighted F1 Score: {weighted_f1}\\nClassification Report:\\n{class_report}\")\n",
    "    print(f\"Validation Cross-Entropy Loss: {avg_cross_entropy_loss}\")\n",
    "\n",
    "    return weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = GraphTransformer(num_nodes, embedding_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Learning rate\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Loss function\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move your model to the chosen device\n",
    "model = model.to(device)\n",
    "\n",
    "def train(model, train_data, train_cluster_features, edge_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(edge_loader, desc='Training'):\n",
    "        edge_tensors, label_batch = batch\n",
    "        \n",
    "        node_u_list, node_v_list = edge_tensors[0].to(device), edge_tensors[1].to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass both train_data and train_cluster_features to the model\n",
    "        output = model(train_data.to(device), train_cluster_features.to(device))\n",
    "        edge_predictions = output[node_u_list]  # Corrected indexing\n",
    "        \n",
    "        loss = criterion(edge_predictions, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(edge_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.15171183995448803\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.04      0.24      0.07       694\n",
      "         expression       0.03      0.08      0.05       722\n",
      "         inhibition       0.10      0.09      0.09      1745\n",
      "binding/association       0.02      0.10      0.04       458\n",
      "    phosphorylation       0.03      0.10      0.04      1024\n",
      "        no_relation       0.33      0.23      0.27      6143\n",
      "         activation       0.42      0.06      0.11      7342\n",
      "\n",
      "           accuracy                           0.13     18128\n",
      "          macro avg       0.14      0.13      0.10     18128\n",
      "       weighted avg       0.30      0.13      0.15     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.9766252173317804\n",
      "Epoch 1, Train Loss: 2.0380, Val F1 Score: 0.1517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.1683099398166673\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.04      0.24      0.07       694\n",
      "         expression       0.04      0.07      0.05       722\n",
      "         inhibition       0.10      0.06      0.08      1745\n",
      "binding/association       0.03      0.10      0.04       458\n",
      "    phosphorylation       0.02      0.06      0.03      1024\n",
      "        no_relation       0.33      0.29      0.31      6143\n",
      "         activation       0.41      0.07      0.12      7342\n",
      "\n",
      "           accuracy                           0.15     18128\n",
      "          macro avg       0.14      0.13      0.10     18128\n",
      "       weighted avg       0.29      0.15      0.17     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.9606052372190688\n",
      "Epoch 2, Train Loss: 2.0132, Val F1 Score: 0.1683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.18162084894128164\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.05      0.28      0.08       694\n",
      "         expression       0.01      0.02      0.02       722\n",
      "         inhibition       0.15      0.06      0.08      1745\n",
      "binding/association       0.02      0.05      0.03       458\n",
      "    phosphorylation       0.02      0.05      0.03      1024\n",
      "        no_relation       0.33      0.35      0.34      6143\n",
      "         activation       0.42      0.08      0.13      7342\n",
      "\n",
      "           accuracy                           0.17     18128\n",
      "          macro avg       0.14      0.13      0.10     18128\n",
      "       weighted avg       0.30      0.17      0.18     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.944985224141015\n",
      "Epoch 3, Train Loss: 1.9941, Val F1 Score: 0.1816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.1976746806298375\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.05      0.29      0.08       694\n",
      "         expression       0.01      0.01      0.01       722\n",
      "         inhibition       0.12      0.04      0.06      1745\n",
      "binding/association       0.02      0.04      0.03       458\n",
      "    phosphorylation       0.02      0.04      0.02      1024\n",
      "        no_relation       0.34      0.41      0.37      6143\n",
      "         activation       0.46      0.09      0.15      7342\n",
      "\n",
      "           accuracy                           0.20     18128\n",
      "          macro avg       0.14      0.13      0.10     18128\n",
      "       weighted avg       0.31      0.20      0.20     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.9298885001076593\n",
      "Epoch 4, Train Loss: 1.9721, Val F1 Score: 0.1977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2105401363286851\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.05      0.33      0.09       694\n",
      "         expression       0.01      0.01      0.01       722\n",
      "         inhibition       0.13      0.04      0.06      1745\n",
      "binding/association       0.02      0.04      0.03       458\n",
      "    phosphorylation       0.01      0.02      0.02      1024\n",
      "        no_relation       0.34      0.46      0.39      6143\n",
      "         activation       0.48      0.10      0.16      7342\n",
      "\n",
      "           accuracy                           0.22     18128\n",
      "          macro avg       0.15      0.14      0.11     18128\n",
      "       weighted avg       0.32      0.22      0.21     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.9151354829470317\n",
      "Epoch 5, Train Loss: 1.9517, Val F1 Score: 0.2105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.22487051143288644\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.05      0.29      0.09       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "         inhibition       0.22      0.03      0.06      1745\n",
      "binding/association       0.03      0.02      0.03       458\n",
      "    phosphorylation       0.01      0.02      0.02      1024\n",
      "        no_relation       0.34      0.54      0.42      6143\n",
      "         activation       0.46      0.11      0.18      7342\n",
      "\n",
      "           accuracy                           0.24     18128\n",
      "          macro avg       0.16      0.15      0.11     18128\n",
      "       weighted avg       0.33      0.24      0.22     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.900701814227634\n",
      "Epoch 6, Train Loss: 1.9310, Val F1 Score: 0.2249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2239954531715866\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.04      0.23      0.07       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "         inhibition       0.08      0.01      0.01      1745\n",
      "binding/association       0.02      0.02      0.02       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "        no_relation       0.34      0.59      0.43      6143\n",
      "         activation       0.46      0.11      0.18      7342\n",
      "\n",
      "           accuracy                           0.25     18128\n",
      "          macro avg       0.14      0.14      0.10     18128\n",
      "       weighted avg       0.31      0.25      0.22     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.8867918716536627\n",
      "Epoch 7, Train Loss: 1.9106, Val F1 Score: 0.2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.23282051892506284\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.04      0.19      0.07       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "         inhibition       0.14      0.01      0.01      1745\n",
      "binding/association       0.01      0.01      0.01       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "        no_relation       0.34      0.62      0.44      6143\n",
      "         activation       0.47      0.12      0.20      7342\n",
      "\n",
      "           accuracy                           0.27     18128\n",
      "          macro avg       0.14      0.14      0.10     18128\n",
      "       weighted avg       0.32      0.27      0.23     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.8731168177392747\n",
      "Epoch 8, Train Loss: 1.8971, Val F1 Score: 0.2328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2378958987114396\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.06      0.19      0.09       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "         inhibition       0.16      0.01      0.01      1745\n",
      "binding/association       0.01      0.01      0.01       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "        no_relation       0.34      0.68      0.45      6143\n",
      "         activation       0.47      0.12      0.20      7342\n",
      "\n",
      "           accuracy                           0.29     18128\n",
      "          macro avg       0.15      0.14      0.11     18128\n",
      "       weighted avg       0.33      0.29      0.24     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.8599629004796345\n",
      "Epoch 9, Train Loss: 1.8768, Val F1 Score: 0.2379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2479807736317506\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.06      0.15      0.08       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "         inhibition       0.18      0.01      0.01      1745\n",
      "binding/association       0.01      0.00      0.00       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "        no_relation       0.34      0.72      0.46      6143\n",
      "         activation       0.49      0.14      0.21      7342\n",
      "\n",
      "           accuracy                           0.31     18128\n",
      "          macro avg       0.15      0.14      0.11     18128\n",
      "       weighted avg       0.33      0.31      0.25     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.8470862706502278\n",
      "Epoch 10, Train Loss: 1.8606, Val F1 Score: 0.2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2451516719013837\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.06      0.13      0.08       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "         inhibition       0.12      0.00      0.01      1745\n",
      "binding/association       0.01      0.00      0.00       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "        no_relation       0.34      0.74      0.47      6143\n",
      "         activation       0.49      0.13      0.20      7342\n",
      "\n",
      "           accuracy                           0.31     18128\n",
      "          macro avg       0.15      0.14      0.11     18128\n",
      "       weighted avg       0.33      0.31      0.25     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.8346104423205059\n",
      "Epoch 11, Train Loss: 1.8435, Val F1 Score: 0.2452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.24236224249153626\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.03      0.05      0.04       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "binding/association       0.01      0.00      0.00       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "        no_relation       0.34      0.78      0.48      6143\n",
      "         activation       0.48      0.12      0.20      7342\n",
      "\n",
      "           accuracy                           0.32     18128\n",
      "          macro avg       0.12      0.14      0.10     18128\n",
      "       weighted avg       0.31      0.32      0.24     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.8226220077938504\n",
      "Epoch 12, Train Loss: 1.8293, Val F1 Score: 0.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2418830277522208\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           compound       0.04      0.05      0.05       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "binding/association       0.01      0.00      0.00       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "        no_relation       0.34      0.81      0.48      6143\n",
      "         activation       0.47      0.12      0.19      7342\n",
      "\n",
      "           accuracy                           0.32     18128\n",
      "          macro avg       0.12      0.14      0.10     18128\n",
      "       weighted avg       0.31      0.32      0.24     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.8111601803037856\n",
      "Epoch 13, Train Loss: 1.8156, Val F1 Score: 0.2419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 11/100 [00:08<01:08,  1.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\yc\\Study doesn't love you\\8735_unsup_learning\\GNN\\GraphTransformer_fuzzy_cluster freeze.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m best_model_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Call train function with necessary parameters\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_data, train_cluster_features, edge_loader, optimizer, criterion, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Call validate function with necessary parameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     val_f1_score \u001b[39m=\u001b[39m validate(model, val_data, val_cluster_features, val_edge_loader, criterion, device)\n",
      "\u001b[1;32mc:\\yc\\Study doesn't love you\\8735_unsup_learning\\GNN\\GraphTransformer_fuzzy_cluster freeze.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Pass both train_data and train_cluster_features to the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m output \u001b[39m=\u001b[39m model(train_data\u001b[39m.\u001b[39mto(device), train_cluster_features\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m edge_predictions \u001b[39m=\u001b[39m output[node_u_list]  \u001b[39m# Corrected indexing\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/yc/Study%20doesn%27t%20love%20you/8735_unsup_learning/GNN/GraphTransformer_fuzzy_cluster%20freeze.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(edge_predictions, label_batch)\n",
      "File \u001b[1;32mc:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\torch_geometric\\data\\data.py:297\u001b[0m, in \u001b[0;36mBaseData.to\u001b[1;34m(self, device, non_blocking, *args)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto\u001b[39m(\u001b[39mself\u001b[39m, device: Union[\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m], \u001b[39m*\u001b[39margs: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    294\u001b[0m        non_blocking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    295\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[39m    only the ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply(\n\u001b[0;32m    298\u001b[0m         \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice, non_blocking\u001b[39m=\u001b[39mnon_blocking), \u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\torch_geometric\\data\\data.py:280\u001b[0m, in \u001b[0;36mBaseData.apply\u001b[1;34m(self, func, *args)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[39mthe ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m store \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstores:\n\u001b[1;32m--> 280\u001b[0m     store\u001b[39m.\u001b[39mapply(func, \u001b[39m*\u001b[39margs)\n\u001b[0;32m    281\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\torch_geometric\\data\\storage.py:191\u001b[0m, in \u001b[0;36mBaseStorage.apply\u001b[1;34m(self, func, *args)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39mthe ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems(\u001b[39m*\u001b[39margs):\n\u001b[1;32m--> 191\u001b[0m     \u001b[39mself\u001b[39m[key] \u001b[39m=\u001b[39m recursive_apply(value, func)\n\u001b[0;32m    192\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\torch_geometric\\data\\storage.py:749\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[1;34m(data, func)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\u001b[39m*\u001b[39m(recursive_apply(d, func) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data))\n\u001b[0;32m    748\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Sequence) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 749\u001b[0m     \u001b[39mreturn\u001b[39;00m [recursive_apply(d, func) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data]\n\u001b[0;32m    750\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m    751\u001b[0m     \u001b[39mreturn\u001b[39;00m {key: recursive_apply(data[key], func) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m data}\n",
      "File \u001b[1;32mc:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\torch_geometric\\data\\storage.py:749\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(data)(\u001b[39m*\u001b[39m(recursive_apply(d, func) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data))\n\u001b[0;32m    748\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Sequence) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 749\u001b[0m     \u001b[39mreturn\u001b[39;00m [recursive_apply(d, func) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data]\n\u001b[0;32m    750\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m    751\u001b[0m     \u001b[39mreturn\u001b[39;00m {key: recursive_apply(data[key], func) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m data}\n",
      "File \u001b[1;32mc:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\torch_geometric\\data\\storage.py:750\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[1;34m(data, func)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Sequence) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    749\u001b[0m     \u001b[39mreturn\u001b[39;00m [recursive_apply(d, func) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data]\n\u001b[1;32m--> 750\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Mapping):\n\u001b[0;32m    751\u001b[0m     \u001b[39mreturn\u001b[39;00m {key: recursive_apply(data[key], func) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m data}\n\u001b[0;32m    752\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m<frozen abc>:119\u001b[0m, in \u001b[0;36m__instancecheck__\u001b[1;34m(cls, instance)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Early stopping and scheduler parameters\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "best_f1_score = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Call train function with necessary parameters\n",
    "    train_loss = train(model, train_data, train_cluster_features, edge_loader, optimizer, criterion, device)\n",
    "\n",
    "    # Call validate function with necessary parameters\n",
    "    val_f1_score = validate(model, val_data, val_cluster_features, val_edge_loader, criterion, device)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val F1 Score: {val_f1_score:.4f}')\n",
    "\n",
    "    # Step the scheduler based on validation F1 score\n",
    "    scheduler.step(val_f1_score)\n",
    "\n",
    "    # Check for improvement in validation F1 score\n",
    "    if val_f1_score > best_f1_score:\n",
    "        best_f1_score = val_f1_score\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()  # Save the best model state\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Save the best model state\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'GTCFFuzzy_32_32_5_best.pth')\n",
    "    print(\"Best model saved.\")\n",
    "else:\n",
    "    print(\"No model improvement was observed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_siamese_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
