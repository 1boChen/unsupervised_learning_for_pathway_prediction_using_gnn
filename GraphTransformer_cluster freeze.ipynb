{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 4816\n",
      "Number of edges: 101782\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import csv\n",
    "\n",
    "def build_multidigraph_from_csv(csv_file):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            # Add nodes (if not already added)\n",
    "            G.add_node(row['starter_ID'], name=row['starter_ID'])\n",
    "            G.add_node(row['receiver_ID'], name=row['receiver_ID'])\n",
    "\n",
    "            # Add directed edges with additional attributes\n",
    "            # Each edge is unique and can represent a different type of interaction\n",
    "            G.add_edge(\n",
    "                row['starter_ID'], \n",
    "                row['receiver_ID'], \n",
    "                interaction_type=row['subtype_name'],\n",
    "                relation_type=row['relation_type'],\n",
    "                pathway_sources=row['pathway_source'],\n",
    "                credibility=row['credibility']\n",
    "            )\n",
    "\n",
    "    return G\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'relations_train_final.csv'  # Update this to the path of your relations_train.csv file\n",
    "\n",
    "# Build the multidigraph\n",
    "MDG = build_multidigraph_from_csv(csv_file_path)\n",
    "\n",
    "# Print basic information about the multidigraph\n",
    "print(f\"Number of nodes: {MDG.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {MDG.number_of_edges()}\")\n",
    "\n",
    "# The multidigraph `MDG` is now constructed with nodes and multiple types of directed edges from the relations_train.csv\n",
    "# You can now use `MDG` for further analysis or as input to clustering algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_cluster_assignments(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    gene_to_cluster = {row['Gene']: row['Cluster'] for _, row in df.iterrows()}\n",
    "    return gene_to_cluster\n",
    "\n",
    "# Use this function to read your CSV file\n",
    "gene_to_cluster = read_cluster_assignments('vgae_kmeans_gene_cluster_assignments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "\n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, num_classes, cluster_feature_dim=8, num_hidden_units=32, num_heads=5):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.node_emb = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "        self.cluster_emb = torch.nn.Embedding(num_clusters, cluster_feature_dim)  # Initialize cluster embeddings\n",
    "\n",
    "        # Freeze the cluster embeddings\n",
    "        self.cluster_emb.weight.requires_grad = False\n",
    "\n",
    "        # First Graph Transformer layer\n",
    "        self.conv1 = TransformerConv(embedding_dim + cluster_feature_dim, num_hidden_units, heads=num_heads, dropout=0.6, edge_dim=None)\n",
    "        \n",
    "        # Output layer\n",
    "        self.conv2 = TransformerConv(num_hidden_units * num_heads, num_classes, heads=1, concat=True, dropout=0.6, edge_dim=None)\n",
    "\n",
    "    def forward(self, data, cluster_indices):\n",
    "        x = self.node_emb(data.node_index)\n",
    "        cluster_x = self.cluster_emb(cluster_indices)  # Embedding for cluster\n",
    "        x = torch.cat([x, cluster_x], dim=1)  # Concatenate node and cluster embeddings\n",
    "\n",
    "        edge_index = data.edge_index\n",
    "        edge_weight = None  # Update if you have edge weights\n",
    "\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index, edge_weight))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Assuming MDG is your MultiDiGraph\n",
    "train_data = from_networkx(MDG)\n",
    "\n",
    "num_nodes = train_data.num_nodes\n",
    "embedding_dim = 32  # Choose an appropriate embedding size\n",
    "num_classes = 7   # Set the number of classes based on your edge types\n",
    "num_clusters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all unique interaction types from MDG\n",
    "interaction_types = set()\n",
    "for _, _, edge_data in MDG.edges(data=True):\n",
    "    interaction_types.add(edge_data['interaction_type'])\n",
    "\n",
    "# Update the mapping to include 'no interaction' class\n",
    "interaction_type_to_label = {inter_type: i for i, inter_type in enumerate(interaction_types)}\n",
    "num_classes = len(interaction_type_to_label)\n",
    "\n",
    "def setup_edge_labels_with_no_interaction(MDG, interaction_type_to_label, data):\n",
    "    num_nodes = len(MDG.nodes())\n",
    "    edge_labels = torch.zeros((num_nodes, num_nodes), dtype=torch.long)\n",
    "\n",
    "    for u, v, edge_data in MDG.edges(data=True):\n",
    "        u_index = list(MDG.nodes()).index(u)\n",
    "        v_index = list(MDG.nodes()).index(v)\n",
    "        label = interaction_type_to_label[edge_data['interaction_type']]\n",
    "        edge_labels[u_index, v_index] = label\n",
    "\n",
    "    data.edge_label = edge_labels  # Directly modify the data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nodes_to_clusters(G, gene_to_cluster):\n",
    "    # Assuming gene_to_cluster is the dictionary from gene to cluster ID\n",
    "    node_to_cluster = {}\n",
    "    for node in G.nodes():\n",
    "        cluster_id = gene_to_cluster.get(node, 99)  # default_cluster_id can be set to a specific value\n",
    "        node_to_cluster[node] = cluster_id\n",
    "\n",
    "    return node_to_cluster\n",
    "\n",
    "node_to_cluster = map_nodes_to_clusters(MDG, gene_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Process training data\n",
    "val_MDG = build_multidigraph_from_csv('cleaned_relations_val_final.csv')\n",
    "\n",
    "# Create a global node to index mapping\n",
    "all_nodes = set(MDG.nodes()).union(set(val_MDG.nodes()))\n",
    "global_node_to_index = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "\n",
    "# Function to map nodes in a graph to global indices\n",
    "def map_nodes_to_global_indices(G, global_node_to_index):\n",
    "    return [global_node_to_index[node] for node in G.nodes()]\n",
    "\n",
    "val_data = from_networkx(val_MDG)\n",
    "\n",
    "train_data.node_index = torch.tensor(map_nodes_to_global_indices(MDG, global_node_to_index), dtype=torch.long)\n",
    "\n",
    "cluster_indices = [node_to_cluster[node] for node in MDG.nodes()]\n",
    "cluster_indices_tensor = torch.tensor(cluster_indices, dtype=torch.long)\n",
    "\n",
    "val_data.node_index = torch.tensor(map_nodes_to_global_indices(val_MDG, global_node_to_index), dtype=torch.long)\n",
    "setup_edge_labels_with_no_interaction(val_MDG, interaction_type_to_label, val_data)\n",
    "setup_edge_labels_with_no_interaction(MDG, interaction_type_to_label, train_data)\n",
    "val_node_to_cluster = map_nodes_to_clusters(val_MDG, gene_to_cluster)\n",
    "\n",
    "# Convert the node-cluster mapping to a tensor for the validation set\n",
    "val_cluster_indices = [val_node_to_cluster[node] for node in val_MDG.nodes()]\n",
    "val_cluster_indices_tensor = torch.tensor(val_cluster_indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "model = GraphTransformer(num_nodes, embedding_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dataset of edges and their labels\n",
    "edges = train_data.edge_index.t().tolist()  # List of [node_u, node_v]\n",
    "labels = [train_data.edge_label[edge[0], edge[1]].item() for edge in edges]\n",
    "\n",
    "edge_dataset = list(zip(edges, labels))\n",
    "edge_loader = DataLoader(edge_dataset, batch_size=1024, shuffle=True)  # Adjust batch_size as needed\n",
    "\n",
    "# Assuming val_data is structured similarly to train_data\n",
    "val_edges = val_data.edge_index.t().tolist()\n",
    "val_labels = [val_data.edge_label[edge[0], edge[1]].item() for edge in val_edges]\n",
    "\n",
    "val_edge_dataset = list(zip(val_edges, val_labels))\n",
    "val_edge_loader = DataLoader(val_edge_dataset, batch_size=1024, shuffle=False)  # You can adjust the batch size\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "inverse_interaction_type_to_label = {v: k for k, v in interaction_type_to_label.items()}\n",
    "\n",
    "def validate(val_data, val_cluster_indices_tensor, device, model, criterion):\n",
    "    model.eval()\n",
    "    total_cross_entropy_loss = 0  # Initialize cross-entropy loss\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_edge_loader, desc='Validating'):\n",
    "            edge_tensors, label_batch = batch\n",
    "            node_u_list, node_v_list = edge_tensors[0].to(device), edge_tensors[1].to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "\n",
    "            # Pass both val_data and val_cluster_indices_tensor to the model\n",
    "            output = model(val_data.to(device), val_cluster_indices_tensor.to(device))\n",
    "            edge_predictions = output[node_u_list].argmax(dim=1)  # Predicted classes\n",
    "\n",
    "            # Calculate and accumulate cross-entropy loss\n",
    "            cross_entropy_loss = criterion(output[node_u_list], label_batch)\n",
    "            total_cross_entropy_loss += cross_entropy_loss.item()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            preds = edge_predictions.cpu().numpy()\n",
    "            true_labels = label_batch.cpu().numpy()\n",
    "            all_predictions.extend(preds)\n",
    "            all_true_labels.extend(true_labels)\n",
    "\n",
    "    # Calculate average cross-entropy loss over all batches\n",
    "    avg_cross_entropy_loss = total_cross_entropy_loss / len(val_edge_loader)\n",
    "\n",
    "    # Calculate F1 Score and classification report\n",
    "    weighted_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "    class_report = classification_report(all_true_labels, all_predictions, target_names=[inverse_interaction_type_to_label[i] for i in range(num_classes)], zero_division=0)\n",
    "\n",
    "    print(f\"Weighted F1 Score: {weighted_f1}\\nClassification Report:\\n{class_report}\")\n",
    "    print(f\"Validation Cross-Entropy Loss: {avg_cross_entropy_loss}\")\n",
    "\n",
    "    return weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = GraphTransformer(num_nodes, embedding_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Learning rate\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Loss function\n",
    "# Check if CUDA (GPU support) is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move your model to the chosen device\n",
    "model = model.to(device)\n",
    "\n",
    "def train(train_data, cluster_indices_tensor, model, optimizer, criterion, edge_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(edge_loader, desc='Training'):\n",
    "        edge_tensors, label_batch = batch\n",
    "        \n",
    "        node_u_list, node_v_list = edge_tensors[0].to(device), edge_tensors[1].to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass both train_data and cluster_indices_tensor to the model\n",
    "        output = model(train_data.to(device), cluster_indices_tensor.to(device))\n",
    "\n",
    "        edge_predictions = output[node_u_list]  # Corrected indexing\n",
    "        \n",
    "        loss = criterion(edge_predictions, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(edge_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:47<00:00,  1.07s/it]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.2852809388645003\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.17      0.23      6143\n",
      "           compound       0.05      0.16      0.07       694\n",
      "         expression       0.03      0.01      0.01       722\n",
      "binding/association       0.02      0.07      0.03       458\n",
      "    phosphorylation       0.04      0.03      0.03      1024\n",
      "         activation       0.43      0.60      0.50      7342\n",
      "\n",
      "           accuracy                           0.31     18128\n",
      "          macro avg       0.13      0.15      0.13     18128\n",
      "       weighted avg       0.30      0.31      0.29     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.82952794763777\n",
      "Epoch 1, Train Loss: 2.0879, Val F1 Score: 0.2853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]\n",
      "Validating: 100%|██████████| 18/18 [00:04<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3195036404296214\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.31      0.33      6143\n",
      "           compound       0.04      0.08      0.06       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.03      0.04      0.03       458\n",
      "    phosphorylation       0.05      0.01      0.02      1024\n",
      "         activation       0.44      0.60      0.51      7342\n",
      "\n",
      "           accuracy                           0.35     18128\n",
      "          macro avg       0.13      0.15      0.13     18128\n",
      "       weighted avg       0.30      0.35      0.32     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7523737682236566\n",
      "Epoch 2, Train Loss: 1.8893, Val F1 Score: 0.3195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [02:29<00:00,  1.49s/it]\n",
      "Validating: 100%|██████████| 18/18 [00:04<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3362972586028845\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.43      0.38      6143\n",
      "           compound       0.02      0.03      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.06      0.04      0.05       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         activation       0.46      0.57      0.51      7342\n",
      "\n",
      "           accuracy                           0.38     18128\n",
      "          macro avg       0.13      0.15      0.14     18128\n",
      "       weighted avg       0.30      0.38      0.34     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7197898493872747\n",
      "Epoch 3, Train Loss: 1.8059, Val F1 Score: 0.3363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [02:28<00:00,  1.49s/it]\n",
      "Validating: 100%|██████████| 18/18 [00:04<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.34733644045542766\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.35      0.46      0.39      6143\n",
      "           compound       0.05      0.03      0.04       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.10      0.03      0.05       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         activation       0.47      0.59      0.52      7342\n",
      "\n",
      "           accuracy                           0.40     18128\n",
      "          macro avg       0.14      0.16      0.14     18128\n",
      "       weighted avg       0.31      0.40      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.7017865180969238\n",
      "Epoch 4, Train Loss: 1.7582, Val F1 Score: 0.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:32<00:00,  1.08it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.35090284845573017\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.46      0.39      6143\n",
      "           compound       0.07      0.03      0.04       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         activation       0.47      0.61      0.53      7342\n",
      "\n",
      "           accuracy                           0.41     18128\n",
      "          macro avg       0.13      0.16      0.14     18128\n",
      "       weighted avg       0.31      0.41      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6894561383459303\n",
      "Epoch 5, Train Loss: 1.7338, Val F1 Score: 0.3509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3491482820507393\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.46      0.39      6143\n",
      "           compound       0.05      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         activation       0.47      0.61      0.53      7342\n",
      "\n",
      "           accuracy                           0.40     18128\n",
      "          macro avg       0.12      0.16      0.14     18128\n",
      "       weighted avg       0.31      0.40      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.680410901705424\n",
      "Epoch 6, Train Loss: 1.7079, Val F1 Score: 0.3491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.34974702368828403\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.47      0.39      6143\n",
      "           compound       0.07      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         activation       0.47      0.61      0.53      7342\n",
      "\n",
      "           accuracy                           0.41     18128\n",
      "          macro avg       0.13      0.16      0.14     18128\n",
      "       weighted avg       0.31      0.41      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6727469960848491\n",
      "Epoch 7, Train Loss: 1.6929, Val F1 Score: 0.3497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.35098125551107806\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.46      0.39      6143\n",
      "           compound       0.07      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         activation       0.47      0.63      0.54      7342\n",
      "\n",
      "           accuracy                           0.41     18128\n",
      "          macro avg       0.13      0.16      0.14     18128\n",
      "       weighted avg       0.31      0.41      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6629633969730802\n",
      "Epoch 8, Train Loss: 1.6758, Val F1 Score: 0.3510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:14<00:00,  1.34it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3522964724586283\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.45      0.39      6143\n",
      "           compound       0.09      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         activation       0.47      0.64      0.54      7342\n",
      "\n",
      "           accuracy                           0.41     18128\n",
      "          macro avg       0.13      0.16      0.14     18128\n",
      "       weighted avg       0.31      0.41      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.655489398373498\n",
      "Epoch 9, Train Loss: 1.6580, Val F1 Score: 0.3523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:14<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3521901120691977\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.46      0.39      6143\n",
      "           compound       0.09      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.00      0.00      0.00      1024\n",
      "         activation       0.47      0.62      0.54      7342\n",
      "\n",
      "           accuracy                           0.41     18128\n",
      "          macro avg       0.13      0.16      0.14     18128\n",
      "       weighted avg       0.31      0.41      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.650462223423852\n",
      "Epoch 10, Train Loss: 1.6450, Val F1 Score: 0.3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.35680455021488106\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.47      0.39      6143\n",
      "           compound       0.10      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.31      0.05      0.09      1024\n",
      "         activation       0.48      0.61      0.54      7342\n",
      "\n",
      "           accuracy                           0.41     18128\n",
      "          macro avg       0.18      0.16      0.15     18128\n",
      "       weighted avg       0.33      0.41      0.36     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.641876094871097\n",
      "Epoch 11, Train Loss: 1.6338, Val F1 Score: 0.3568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3523364918950407\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.00      0.00      0.00      1745\n",
      "        no_relation       0.34      0.41      0.38      6143\n",
      "           compound       0.06      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.29      0.05      0.09      1024\n",
      "         activation       0.46      0.65      0.54      7342\n",
      "\n",
      "           accuracy                           0.41     18128\n",
      "          macro avg       0.16      0.16      0.15     18128\n",
      "       weighted avg       0.32      0.41      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6338738467958238\n",
      "Epoch 12, Train Loss: 1.6231, Val F1 Score: 0.3523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3557224697191228\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.09      0.00      0.00      1745\n",
      "        no_relation       0.34      0.42      0.38      6143\n",
      "           compound       0.16      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.31      0.11      0.16      1024\n",
      "         activation       0.46      0.64      0.54      7342\n",
      "\n",
      "           accuracy                           0.41     18128\n",
      "          macro avg       0.20      0.17      0.16     18128\n",
      "       weighted avg       0.34      0.41      0.36     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6282564004262288\n",
      "Epoch 13, Train Loss: 1.6133, Val F1 Score: 0.3557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.35240440123800376\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.09      0.00      0.00      1745\n",
      "        no_relation       0.34      0.45      0.39      6143\n",
      "           compound       0.14      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.31      0.11      0.16      1024\n",
      "         activation       0.46      0.60      0.52      7342\n",
      "\n",
      "           accuracy                           0.40     18128\n",
      "          macro avg       0.19      0.17      0.16     18128\n",
      "       weighted avg       0.33      0.40      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6234848499298096\n",
      "Epoch 14, Train Loss: 1.6042, Val F1 Score: 0.3524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.35it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3527075435294619\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.09      0.00      0.00      1745\n",
      "        no_relation       0.34      0.41      0.37      6143\n",
      "           compound       0.14      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.31      0.11      0.16      1024\n",
      "         activation       0.45      0.64      0.53      7342\n",
      "\n",
      "           accuracy                           0.41     18128\n",
      "          macro avg       0.19      0.17      0.16     18128\n",
      "       weighted avg       0.33      0.41      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6152955624792311\n",
      "Epoch 15, Train Loss: 1.5960, Val F1 Score: 0.3527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [01:13<00:00,  1.37it/s]\n",
      "Validating: 100%|██████████| 18/18 [00:02<00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 Score: 0.3515829949280002\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "         inhibition       0.09      0.00      0.00      1745\n",
      "        no_relation       0.35      0.38      0.36      6143\n",
      "           compound       0.14      0.02      0.03       694\n",
      "         expression       0.00      0.00      0.00       722\n",
      "binding/association       0.00      0.00      0.00       458\n",
      "    phosphorylation       0.30      0.11      0.16      1024\n",
      "         activation       0.45      0.67      0.54      7342\n",
      "\n",
      "           accuracy                           0.41     18128\n",
      "          macro avg       0.19      0.17      0.16     18128\n",
      "       weighted avg       0.33      0.41      0.35     18128\n",
      "\n",
      "Validation Cross-Entropy Loss: 1.6086680955357022\n",
      "Epoch 16, Train Loss: 1.5871, Val F1 Score: 0.3516\n",
      "Early stopping triggered\n",
      "Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Early stopping and scheduler parameters\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "best_f1_score = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Call train function with necessary parameters\n",
    "    train_loss = train(train_data, cluster_indices_tensor, model, optimizer, criterion, edge_loader, device)\n",
    "\n",
    "    # Call validate function with necessary parameters\n",
    "    val_f1_score = validate(val_data, val_cluster_indices_tensor, device, model, criterion)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val F1 Score: {val_f1_score:.4f}')\n",
    "\n",
    "    # Step the scheduler based on validation F1 score\n",
    "    scheduler.step(val_f1_score)\n",
    "\n",
    "    # Check for improvement in validation F1 score\n",
    "    if val_f1_score > best_f1_score:\n",
    "        best_f1_score = val_f1_score\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()  # Save the best model state\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Save the best model state\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'new_GT_Freeze_Kmeans_32_32_5_best.pth')\n",
    "    print(\"Best model saved.\")\n",
    "else:\n",
    "    print(\"No model improvement was observed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_siamese_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
