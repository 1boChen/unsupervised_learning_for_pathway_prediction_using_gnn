{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '6'  # Adjust the number of threads as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def build_multidigraph_from_csv(csv_file):\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            # Exclude 'no_relation' edges\n",
    "            if row['relation_type'] != 'no_relation':\n",
    "                # Add nodes with the 'name' attribute\n",
    "                G.add_node(row['starter_ID'], name=row['starter_ID'])\n",
    "                G.add_node(row['receiver_ID'], name=row['receiver_ID'])\n",
    "\n",
    "                # Add directed edges with additional attributes\n",
    "                weight = float(row['weight'])\n",
    "                G.add_edge(\n",
    "                    row['starter_ID'], \n",
    "                    row['receiver_ID'], \n",
    "                    weight=weight,\n",
    "                    interaction_type=row['subtype_name'],\n",
    "                    relation_type=row['relation_type'],\n",
    "                    pathway_sources=row['pathway_source'],\n",
    "                    credibility=row['credibility']\n",
    "                )\n",
    "    \n",
    "    return G\n",
    "\n",
    "def create_global_node_to_index_mapping(train_graph, val_graph):\n",
    "    all_nodes = set(train_graph.nodes()).union(set(val_graph.nodes()))\n",
    "    return {node: i for i, node in enumerate(all_nodes)}\n",
    "\n",
    "# Paths to the CSV files\n",
    "train_csv_path = 'relations_train_final.csv'\n",
    "val_csv_path = 'cleaned_relations_val_final.csv'\n",
    "# Build the MultiDiGraphs\n",
    "train_MDG = build_multidigraph_from_csv(train_csv_path)\n",
    "val_MDG = build_multidigraph_from_csv(val_csv_path)\n",
    "\n",
    "# Create a global node to index mapping\n",
    "global_node_to_index = create_global_node_to_index_mapping(train_MDG, val_MDG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities detected in the training MDG: 92\n",
      "Number of communities detected in the val MDG: 69\n"
     ]
    }
   ],
   "source": [
    "import infomap\n",
    "\n",
    "def apply_infomap(graph):\n",
    "    # Initialize Infomap\n",
    "    im = infomap.Infomap(\"--directed\")\n",
    "\n",
    "    # Create a mapping of node names to integers\n",
    "    node_to_int = {node: i for i, node in enumerate(graph.nodes())}\n",
    "    int_to_node = {i: node for node, i in node_to_int.items()}\n",
    "\n",
    "    # Add nodes and edges to the Infomap network\n",
    "    for node in graph.nodes():\n",
    "        im.add_node(node_to_int[node])\n",
    "    for u, v in graph.edges():\n",
    "        im.add_link(node_to_int[u], node_to_int[v])\n",
    "\n",
    "    # Run the Infomap community detection\n",
    "    im.run()\n",
    "\n",
    "    # Extract the communities\n",
    "    communities = {int_to_node[node.node_id]: node.module_id for node in im.nodes}\n",
    "\n",
    "    return communities\n",
    "\n",
    "# Apply Infomap to your directed graph\n",
    "communities = apply_infomap(train_MDG)\n",
    "val_communities = apply_infomap(val_MDG)\n",
    "\n",
    "# Analyze the results\n",
    "num_communities = len(set(communities.values()))\n",
    "num_val_communities = len(set(val_communities.values()))\n",
    "print(f\"Number of communities detected in the training MDG: {num_communities}\")\n",
    "print(f\"Number of communities detected in the val MDG: {num_val_communities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "def apply_mapping_and_get_indices(graph, mapping):\n",
    "    # Create a tensor of node indices based on the global mapping\n",
    "    num_nodes = len(mapping)\n",
    "    node_indices = torch.arange(num_nodes)\n",
    "\n",
    "    # Remap nodes in the graph according to the global mapping\n",
    "    remapped_graph = nx.relabel_nodes(graph, mapping)\n",
    "\n",
    "    return remapped_graph, node_indices\n",
    "\n",
    "# Apply mapping to training and validation graphs\n",
    "remapped_train_MDG, train_indices = apply_mapping_and_get_indices(train_MDG, global_node_to_index)\n",
    "remapped_val_MDG, val_indices = apply_mapping_and_get_indices(val_MDG, global_node_to_index)\n",
    "\n",
    "# Convert to PyTorch Geometric Data\n",
    "train_data = from_networkx(remapped_train_MDG)\n",
    "train_data.x = train_indices\n",
    "\n",
    "val_data = from_networkx(remapped_val_MDG)\n",
    "val_data.x = val_indices\n",
    "\n",
    "train_data.x = train_data.x.long()  # Convert to LongTensor\n",
    "val_data.x = val_data.x.long()  # Convert to LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "from torch_geometric.nn.models import GAE\n",
    "\n",
    "class GATGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, max_nodes, embedding_dim, out_channels):\n",
    "        super(GATGCNEncoder, self).__init__()\n",
    "        self.node_emb = torch.nn.Embedding(max_nodes, embedding_dim)\n",
    "\n",
    "        # First layer is GCN\n",
    "        self.conv1 = GCNConv(embedding_dim, 2 * out_channels)\n",
    "        \n",
    "        # Second layer is GAT\n",
    "        self.conv2 = GATConv(2 * out_channels, out_channels, heads=1, dropout=0.2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.node_emb(x)  # x is now node indices\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "# Usage\n",
    "max_nodes = 5000  # Set to a number higher than your expected number of nodes\n",
    "embedding_dim = 16\n",
    "out_channels = 16\n",
    "\n",
    "encoder = GATGCNEncoder(max_nodes, embedding_dim, out_channels)\n",
    "model = GAE(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "\n",
    "def validate(model, val_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode the validation data\n",
    "        z_val = model.encode(val_data.x, val_data.edge_index)\n",
    "\n",
    "        # Calculate the reconstruction loss\n",
    "        val_loss = model.recon_loss(z_val, val_data.edge_index)\n",
    "\n",
    "    return val_loss.item()\n",
    "\n",
    "def broad_search(embeddings, step, max_clusters):\n",
    "    best_score = -1\n",
    "    best_n_clusters = 0\n",
    "    for n_clusters in range(2, max_clusters + 1, step):\n",
    "        score = calculate_silhouette_score(embeddings, n_clusters)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n_clusters = n_clusters\n",
    "    return best_score, best_n_clusters\n",
    "\n",
    "def detailed_search(embeddings, start, end, step):\n",
    "    best_score = -1\n",
    "    best_n_clusters = 0\n",
    "    for n_clusters in range(start, end + 1, step):\n",
    "        score = calculate_silhouette_score(embeddings, n_clusters)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_n_clusters = n_clusters\n",
    "    return best_score, best_n_clusters\n",
    "\n",
    "def calculate_silhouette_score(embeddings, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    return silhouette_score(embeddings, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.6981, Val Loss: 1.6455\n",
      "Epoch: 2, Loss: 1.4270, Val Loss: 1.5079\n",
      "Epoch: 3, Loss: 1.3376, Val Loss: 1.4548\n",
      "Epoch: 4, Loss: 1.3223, Val Loss: 1.4393\n",
      "Epoch: 5, Loss: 1.3106, Val Loss: 1.4387\n",
      "Epoch: 6, Loss: 1.3036, Val Loss: 1.4335\n",
      "Epoch: 7, Loss: 1.2900, Val Loss: 1.4346\n",
      "Epoch: 8, Loss: 1.2818, Val Loss: 1.4299\n",
      "Epoch: 9, Loss: 1.2578, Val Loss: 1.4169\n",
      "Epoch: 10, Loss: 1.2504, Val Loss: 1.3958\n",
      "Epoch: 11, Loss: 1.2331, Val Loss: 1.3833\n",
      "Epoch: 12, Loss: 1.2171, Val Loss: 1.3745\n",
      "Epoch: 13, Loss: 1.1938, Val Loss: 1.3747\n",
      "Epoch: 14, Loss: 1.1852, Val Loss: 1.3738\n",
      "Epoch: 15, Loss: 1.1765, Val Loss: 1.3730\n",
      "Epoch: 16, Loss: 1.1562, Val Loss: 1.3818\n",
      "Epoch: 17, Loss: 1.1449, Val Loss: 1.3811\n",
      "Epoch: 18, Loss: 1.1403, Val Loss: 1.3890\n",
      "Epoch: 19, Loss: 1.1235, Val Loss: 1.3969\n",
      "Epoch: 20, Loss: 1.1225, Val Loss: 1.4065\n",
      "Epoch: 21, Loss: 1.1155, Val Loss: 1.4069\n",
      "Epoch: 22, Loss: 1.1004, Val Loss: 1.4180\n",
      "Epoch: 23, Loss: 1.1046, Val Loss: 1.4144\n",
      "Epoch: 24, Loss: 1.0984, Val Loss: 1.4170\n",
      "Epoch: 25, Loss: 1.0936, Val Loss: 1.4202\n",
      "Early stopping triggered\n",
      "Best model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Early stopping and model saving parameters\n",
    "patience = 10\n",
    "best_val_score = -1\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "best_model_state = None  # To store the best model state\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    val_loss = validate(model, val_data)  # Updated to use the new validate function\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    scheduler.step(val_loss)  # Update based on val_loss\n",
    "\n",
    "    # Check for improvement based on decreased loss\n",
    "    if val_loss < best_val_score or best_val_score == -1:\n",
    "        best_val_score = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, 'best_gae_model_recon_loss.pth')\n",
    "    print(\"Best model saved.\")\n",
    "else:\n",
    "    print(\"No model improvement was observed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ycy6y\\.conda\\envs\\pathway_siamese_network\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_gae_model_recon_loss.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    embeddings = z.cpu().numpy()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 92\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "index_to_gene = {index: gene for gene, index in global_node_to_index.items()}\n",
    "\n",
    "gene_names = [index_to_gene[i] for i in range(len(embeddings))]\n",
    "\n",
    "# Combine gene names with their cluster labels\n",
    "gene_cluster_pairs = list(zip(gene_names, cluster_labels))\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('gene_cluster_assignments.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Gene', 'Cluster'])\n",
    "    for gene, cluster in gene_cluster_pairs:\n",
    "        writer.writerow([gene, cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cluster_pairs = [(node, communities[node]) for node in train_MDG.nodes()]\n",
    "\n",
    "import csv\n",
    "\n",
    "# Read the existing CSV file\n",
    "existing_data = {}\n",
    "with open('gene_cluster_assignments.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)  # Skip the header\n",
    "    for row in reader:\n",
    "        gene = row[0]\n",
    "        cluster = row[1]\n",
    "        existing_data[gene] = {'Cluster': cluster, 'Infomap': None}\n",
    "\n",
    "# Add Infomap cluster assignments\n",
    "for gene, infomap_cluster in gene_cluster_pairs:\n",
    "    if gene in existing_data:\n",
    "        existing_data[gene]['Infomap'] = infomap_cluster\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "with open('updated_gene_cluster_assignments.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header + ['Infomap'])  # New header with 'Infomap'\n",
    "    for gene, data in existing_data.items():\n",
    "        writer.writerow([gene, data['Cluster'], data['Infomap']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_siamese_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
